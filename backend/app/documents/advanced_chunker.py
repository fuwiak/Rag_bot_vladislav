"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunker —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –∏ fallback-–∞–º–∏
–†–µ–∞–ª–∏–∑—É–µ—Ç 5 —Ç–µ—Ö–Ω–∏–∫ chunking: Page-Level, Element-Based, Recursive, Semantic, LLM-Based
"""
from typing import List, Dict, Any, Optional, Tuple
import logging
import re
import asyncio

logger = logging.getLogger(__name__)


class AdvancedChunker:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunker —Å fallback-—Ü–µ–ø–æ—á–∫–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):
    1. Page-Level Chunking - –¥–ª—è PDF (—Ç–æ—á–Ω–æ—Å—Ç—å 0.648 –ø–æ NVIDIA)
    2. Element-Based Chunking - —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    3. Recursive Chunking - —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ Markdown
    4. Semantic Chunking - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
    5. LLM-Based Chunking - LLM —Ä–µ—à–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã
    6. Fallback - –ø—Ä–æ—Å—Ç–æ–π chunking –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    """
    
    def __init__(
        self,
        default_chunk_size: int = 800,
        default_overlap: int = 200,
        min_chunk_size: int = 100,
        max_chunk_size: int = 2000
    ):
        """
        Args:
            default_chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            default_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            max_chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        """
        self.default_chunk_size = default_chunk_size
        self.default_overlap = default_overlap
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
    
    async def chunk_document(
        self,
        text: str,
        file_type: str = "txt",
        file_content: Optional[bytes] = None,
        filename: Optional[str] = None
    ) -> List[str]:
        """
        –†–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fallback-—Ü–µ–ø–æ—á–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            file_type: –¢–∏–ø —Ñ–∞–π–ª–∞ (pdf, docx, txt)
            file_content: –°—ã—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ (–¥–ª—è PDF)
            filename: –ò–º—è —Ñ–∞–π–ª–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        logger.info(f"[CHUNKING] üöÄ –ù–∞—á–∞–ª–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {filename or 'unknown'}, —Ç–∏–ø: {file_type}, —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if not text or not text.strip():
            logger.warning("[CHUNKING] ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω chunker'—É")
            return []
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: Page-Level Chunking –¥–ª—è PDF
        if file_type == "pdf" and file_content:
            logger.info(f"[CHUNKING] üìÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä–æ–±—É–µ–º Page-Level Chunking –¥–ª—è PDF (—Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(file_content) / 1024:.2f} KB)")
            chunks = await self._try_page_level_chunking(file_content, text)
            if chunks and len(chunks) > 0:
                avg_chunk_size = sum(len(c) for c in chunks) / len(chunks)
                logger.info(f"[CHUNKING] ‚úÖ Page-Level Chunking —É—Å–ø–µ—à–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                return chunks
            logger.warning("[CHUNKING] ‚ö†Ô∏è Page-Level Chunking –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: Element-Based Chunking
        logger.info(f"[CHUNKING] üìã –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü—Ä–æ–±—É–µ–º Element-Based Chunking")
        chunks = await self._try_element_based_chunking(text, file_type)
        if chunks and len(chunks) > 0:
            avg_chunk_size = sum(len(c) for c in chunks) / len(chunks)
            logger.info(f"[CHUNKING] ‚úÖ Element-Based Chunking —É—Å–ø–µ—à–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            return chunks
        logger.warning("[CHUNKING] ‚ö†Ô∏è Element-Based Chunking –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: Recursive Chunking
        logger.info(f"[CHUNKING] üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü—Ä–æ–±—É–µ–º Recursive Chunking")
        chunks = await self._try_recursive_chunking(text)
        if chunks and len(chunks) > 0:
            avg_chunk_size = sum(len(c) for c in chunks) / len(chunks)
            logger.info(f"[CHUNKING] ‚úÖ Recursive Chunking —É—Å–ø–µ—à–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            return chunks
        logger.warning("[CHUNKING] ‚ö†Ô∏è Recursive Chunking –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: Semantic Chunking
        logger.info(f"[CHUNKING] üß† –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü—Ä–æ–±—É–µ–º Semantic Chunking")
        chunks = await self._try_semantic_chunking(text)
        if chunks and len(chunks) > 0:
            avg_chunk_size = sum(len(c) for c in chunks) / len(chunks)
            logger.info(f"[CHUNKING] ‚úÖ Semantic Chunking —É—Å–ø–µ—à–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            return chunks
        logger.warning("[CHUNKING] ‚ö†Ô∏è Semantic Chunking –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: LLM-Based Chunking (—Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        if len(text) > 10000:
            logger.info(f"[CHUNKING] ü§ñ –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü—Ä–æ–±—É–µ–º LLM-Based Chunking (–±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            chunks = await self._try_llm_based_chunking(text)
            if chunks and len(chunks) > 0:
                avg_chunk_size = sum(len(c) for c in chunks) / len(chunks)
                logger.info(f"[CHUNKING] ‚úÖ LLM-Based Chunking —É—Å–ø–µ—à–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                return chunks
            logger.warning("[CHUNKING] ‚ö†Ô∏è LLM-Based Chunking –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–±—É–µ–º fallback")
        
        # Fallback: –ü—Ä–æ—Å—Ç–æ–π chunking –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        logger.info(f"[CHUNKING] üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: –ø—Ä–æ—Å—Ç–æ–π chunking –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (chunk_size={self.default_chunk_size}, overlap={self.default_overlap})")
        chunks = self._fallback_simple_chunking(text)
        if chunks:
            avg_chunk_size = sum(len(c) for c in chunks) / len(chunks)
            logger.info(f"[CHUNKING] ‚úÖ Fallback chunking –∑–∞–≤–µ—Ä—à–µ–Ω: {len(chunks)} —á–∞–Ω–∫–æ–≤, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        return chunks
    
    async def _try_page_level_chunking(
        self,
        file_content: bytes,
        text: str
    ) -> List[str]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: Page-Level Chunking
        –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ PDF –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º, –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        –¢–æ—á–Ω–æ—Å—Ç—å 0.648 –ø–æ NVIDIA, –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Ç–∞–±–ª–∏—Ü –∏ —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        """
        try:
            import PyPDF2
            import io
            
            logger.info(f"[CHUNKING] üìÑ Page-Level: –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ PDF")
            pdf_file = io.BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            total_pages = len(pdf_reader.pages)
            
            logger.info(f"[CHUNKING] üìÑ Page-Level: –ù–∞–π–¥–µ–Ω–æ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ PDF")
            
            if total_pages == 0:
                logger.warning("[CHUNKING] üìÑ Page-Level: PDF –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü")
                return []
            
            page_chunks = []
            
            for i, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text and page_text.strip():
                        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        page_text = self._clean_text(page_text)
                        if len(page_text) >= self.min_chunk_size:
                            page_chunks.append(f"[–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}]\n{page_text}")
                            if (i + 1) % 10 == 0:
                                logger.info(f"[CHUNKING] üìÑ Page-Level: –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–æ–∑–¥–∞–Ω–æ {len(page_chunks)} —á–∞–Ω–∫–æ–≤")
                        elif page_text.strip():
                            # –ú–∞–ª–µ–Ω—å–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π
                            if page_chunks:
                                page_chunks[-1] += f"\n\n[–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}]\n{page_text}"
                            else:
                                page_chunks.append(f"[–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}]\n{page_text}")
                except Exception as e:
                    logger.warning(f"[CHUNKING] üìÑ Page-Level: –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i+1}: {e}")
                    continue
            
            if page_chunks:
                avg_size = sum(len(c) for c in page_chunks) / len(page_chunks)
                logger.info(f"[CHUNKING] üìÑ Page-Level: ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(page_chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                return page_chunks
            else:
                logger.warning("[CHUNKING] üìÑ Page-Level: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —á–∞–Ω–∫–∏ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü")
            
        except Exception as e:
            logger.warning(f"[CHUNKING] üìÑ Page-Level Chunking failed: {e}")
        
        return []
    
    async def _try_element_based_chunking(
        self,
        text: str,
        file_type: str
    ) -> List[str]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: Element-Based Chunking
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ç–∞–±–ª–∏—Ü—ã, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã)
        –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å—à—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞/—Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        try:
            chunks = []
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (H1-H6 —Å—Ç–∏–ª—å)
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            heading_patterns = [
                r'^#{1,6}\s+.+$',  # Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
                r'^[–ê-–Ø–Å][–ê-–Ø–Å\s]{2,50}$',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                r'^\d+\.\s+[–ê-–Ø–Å]',  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                r'^[–ê-–Ø–Å][–∞-—è—ë\s]{5,100}:$',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–≤–æ–µ—Ç–æ—á–∏–µ–º
            ]
            
            lines = text.split('\n')
            current_chunk = []
            current_heading = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    if current_chunk:
                        current_chunk.append('')
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                is_heading = False
                for pattern in heading_patterns:
                    if re.match(pattern, line, re.MULTILINE):
                        is_heading = True
                        break
                
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                if not is_heading:
                    # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                    if len(line) < 80 and line.isupper() and len(line.split()) < 10:
                        is_heading = True
                
                if is_heading:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
                    if current_chunk:
                        chunk_text = '\n'.join(current_chunk).strip()
                        if len(chunk_text) >= self.min_chunk_size:
                            chunks.append(chunk_text)
                        elif chunks and len(chunk_text) > 0:
                            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —á–∞–Ω–∫–æ–º
                            chunks[-1] += f"\n\n{chunk_text}"
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                    current_heading = line
                    current_chunk = [line]
                else:
                    current_chunk.append(line)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunk_text = '\n'.join(current_chunk).strip()
                if len(chunk_text) >= self.min_chunk_size:
                    chunks.append(chunk_text)
                elif chunks and len(chunk_text) > 0:
                    chunks[-1] += f"\n\n{chunk_text}"
            
            # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ä–æ—à–∏–µ —á–∞–Ω–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
            if chunks and len(chunks) > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ: —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑—É–º–Ω—ã–º
                avg_size = sum(len(c) for c in chunks) / len(chunks)
                if self.min_chunk_size <= avg_size <= self.max_chunk_size:
                    return chunks
            
        except Exception as e:
            logger.warning(f"Element-Based Chunking failed: {e}")
        
        return []
    
    async def _try_recursive_chunking(self, text: str) -> List[str]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: Recursive Chunking
        –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏ –≤ Markdown
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF‚Üítext
        """
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Markdown-–ø–æ–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            markdown_text = self._convert_to_markdown(text)
            
            # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –¥–µ–ª–µ–Ω–∏—è
            separators = [
                '\n\n\n',  # –¢—Ä–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
                '\n\n',    # –î–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
                '\n---\n',  # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è
                '\n# ',     # –ó–∞–≥–æ–ª–æ–≤–æ–∫ Markdown
                '\n## ',
                '\n### ',
                '\n',       # –û–¥–∏–Ω–æ—á–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
                '. ',       # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            ]
            
            chunks = self._recursive_split(markdown_text, separators, 0)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
            filtered_chunks = []
            for chunk in chunks:
                chunk = chunk.strip()
                if len(chunk) >= self.min_chunk_size:
                    filtered_chunks.append(chunk)
                elif filtered_chunks and len(chunk) > 0:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
                    filtered_chunks[-1] += f"\n\n{chunk}"
            
            if filtered_chunks and len(filtered_chunks) > 0:
                return filtered_chunks
            
        except Exception as e:
            logger.warning(f"Recursive Chunking failed: {e}")
        
        return []
    
    def _recursive_split(
        self,
        text: str,
        separators: List[str],
        separator_index: int
    ) -> List[str]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º"""
        if separator_index >= len(separators):
            # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(text) <= self.max_chunk_size:
                return [text]
            # –ò–Ω–∞—á–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∏–º
            return self._force_split(text)
        
        separator = separators[separator_index]
        parts = text.split(separator)
        
        # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–ª–æ —Ö–æ—Ä–æ—à–∏–µ —á–∞—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if len(parts) > 1:
            chunks = []
            for part in parts:
                part = part.strip()
                if not part:
                    continue
                
                if len(part) <= self.max_chunk_size:
                    chunks.append(part)
                else:
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–µ–ª–∏–º –¥–∞–ª—å—à–µ
                    sub_chunks = self._recursive_split(part, separators, separator_index + 1)
                    chunks.extend(sub_chunks)
            
            if chunks:
                return chunks
        
        # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –Ω–µ –ø–æ–º–æ–≥, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π
        return self._recursive_split(text, separators, separator_index + 1)
    
    async def _try_semantic_chunking(self, text: str) -> List[str]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: Semantic Chunking
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ embeddings
        –£–ª—É—á—à–∞–µ—Ç recall –Ω–∞ 9% –≤ —Ç–µ—Å—Ç–∞—Ö Chroma
        """
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = self._split_into_sentences(text)
            
            if len(sentences) < 2:
                return []
            
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å embeddings –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–∏—è
            # –ï—Å–ª–∏ embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
            
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ 1: –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
            # (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –¥–ª–∏–Ω—É)
            chunks = []
            current_chunk = []
            current_chunk_size = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                sentence_len = len(sentence)
                
                # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç max_chunk_size
                if current_chunk_size + sentence_len <= self.max_chunk_size:
                    current_chunk.append(sentence)
                    current_chunk_size += sentence_len
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                    if current_chunk:
                        chunk_text = ' '.join(current_chunk)
                        if len(chunk_text) >= self.min_chunk_size:
                            chunks.append(chunk_text)
                        current_chunk = []
                        current_chunk_size = 0
                    
                    # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–∞–º–æ –ø–æ —Å–µ–±–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ, –¥–µ–ª–∏–º –µ–≥–æ
                    if sentence_len > self.max_chunk_size:
                        sub_chunks = self._force_split(sentence)
                        chunks.extend(sub_chunks)
                    else:
                        current_chunk.append(sentence)
                        current_chunk_size = sentence_len
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text) >= self.min_chunk_size:
                    chunks.append(chunk_text)
            
            if chunks and len(chunks) > 0:
                return chunks
            
        except Exception as e:
            logger.warning(f"Semantic Chunking failed: {e}")
        
        return []
    
    async def _try_llm_based_chunking(self, text: str) -> List[str]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: LLM-Based/Agentic Chunking
        LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Ä–µ—à–∞–µ—Ç –æ –≥—Ä–∞–Ω–∏—Ü–∞—Ö
        –õ—É—á—à–µ–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –Ω–æ –¥–æ—Ä–æ–≥–æ–µ
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM —Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if len(text) < 5000:
                return []
            
            from app.llm.openrouter_client import OpenRouterClient
            from app.core.config import settings
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏.
–î–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏ (—Ä–∞–∑–¥–µ–ª—ã, —Ç–µ–º—ã, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã).
–ö–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 500-1500 —Å–∏–º–≤–æ–ª–æ–≤.

–î–æ–∫—É–º–µ–Ω—Ç:
{text[:10000]}...

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞ —Å–∏–º–≤–æ–ª–æ–≤, –≥–¥–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —á–∞–Ω–∫–æ–≤ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é).
–ù–∞–ø—Ä–∏–º–µ—Ä: 500, 1200, 2500, 4000
–ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É."""

            llm_client = OpenRouterClient(
                model_primary=settings.OPENROUTER_MODEL_PRIMARY,
                model_fallback=settings.OPENROUTER_MODEL_FALLBACK
            )
            
            response = await llm_client.chat_completion(
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–º–æ–≥–∏ —Ä–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.3
            )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç LLM
            boundaries = []
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
                numbers = re.findall(r'\d+', response)
                boundaries = [int(n) for n in numbers if int(n) < len(text)]
                boundaries.sort()
            except:
                pass
            
            if boundaries:
                chunks = []
                start = 0
                for boundary in boundaries:
                    if boundary > start:
                        chunk = text[start:boundary].strip()
                        if len(chunk) >= self.min_chunk_size:
                            chunks.append(chunk)
                        start = boundary
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
                if start < len(text):
                    chunk = text[start:].strip()
                    if len(chunk) >= self.min_chunk_size:
                        chunks.append(chunk)
                
                if chunks:
                    logger.info(f"LLM-Based: —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
                    return chunks
            
        except Exception as e:
            logger.warning(f"LLM-Based Chunking failed: {e}")
        
        return []
    
    def _fallback_simple_chunking(self, text: str) -> List[str]:
        """
        Fallback: –ü—Ä–æ—Å—Ç–æ–π chunking –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –µ—Å–ª–∏ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.default_chunk_size
            
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(text):
                for delimiter in ['. ', '.\n', '! ', '!\n', '? ', '?\n', '\n\n']:
                    last_delimiter = text.rfind(delimiter, start, end)
                    if last_delimiter != -1:
                        end = last_delimiter + len(delimiter)
                        break
            
            chunk = text[start:end].strip()
            if chunk and len(chunk) >= self.min_chunk_size:
                chunks.append(chunk)
            
            # –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —á–∞–Ω–∫—É —Å —É—á–µ—Ç–æ–º overlap
            start = end - self.default_overlap
            if start >= len(text):
                break
        
        return chunks if chunks else [text]
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r' +', ' ', text)
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()
    
    def _convert_to_markdown(self, text: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ Markdown-–ø–æ–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ Markdown
        lines = text.split('\n')
        markdown_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                markdown_lines.append('')
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
            if len(line) < 80 and (line.isupper() or line.endswith(':')):
                markdown_lines.append(f"## {line}")
            else:
                markdown_lines.append(line)
        
        return '\n'.join(markdown_lines)
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
        sentences = re.split(r'[.!?]+\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _force_split(self, text: str) -> List[str]:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + self.max_chunk_size, len(text))
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            start = end
        
        return chunks
    
    async def chunk_large_document(
        self,
        text: str,
        file_type: str = "pdf",
        file_content: Optional[bytes] = None,
        filename: Optional[str] = None,
        use_hierarchical: bool = True
    ) -> Dict[str, Any]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç (200+ —Å—Ç—Ä–∞–Ω–∏—Ü) —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            file_type: –¢–∏–ø —Ñ–∞–π–ª–∞
            file_content: –°—ã—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            use_hierarchical: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —á–∞–Ω–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
            - chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
            - sections: –°–ø–∏—Å–æ–∫ —Å–µ–∫—Ü–∏–π –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
            - hierarchy: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            - metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        result = {
            'chunks': [],
            'sections': [],
            'hierarchy': {},
            'metadata': {
                'total_length': len(text),
                'filename': filename,
                'file_type': file_type,
                'chunking_strategy': 'hierarchical' if use_hierarchical else 'flat'
            }
        }
        
        if not text or not text.strip():
            return result
        
        try:
            # –î–ª—è PDF –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            if file_type == "pdf" and file_content:
                pages = await self._extract_pdf_structure(file_content)
                if pages:
                    result['metadata']['pages'] = len(pages)
                    result['hierarchy']['pages'] = pages
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
            sections = self._extract_sections(text)
            result['sections'] = sections
            
            if use_hierarchical and len(sections) > 1:
                # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —Å–µ–∫—Ü–∏—è–º, –ø–æ—Ç–æ–º –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–π
                all_chunks = []
                for i, section in enumerate(sections):
                    section_chunks = await self.chunk_document(
                        text=section['content'],
                        file_type=file_type
                    )
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
                    for j, chunk in enumerate(section_chunks):
                        all_chunks.append({
                            'text': chunk,
                            'section_index': i,
                            'section_title': section.get('title', f'–°–µ–∫—Ü–∏—è {i+1}'),
                            'chunk_index_in_section': j
                        })
                
                result['chunks'] = all_chunks
                result['metadata']['total_chunks'] = len(all_chunks)
            else:
                # –ü–ª–æ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
                chunks = await self.chunk_document(
                    text=text,
                    file_type=file_type,
                    file_content=file_content,
                    filename=filename
                )
                result['chunks'] = [{'text': c, 'section_index': 0} for c in chunks]
                result['metadata']['total_chunks'] = len(chunks)
            
            logger.info(f"[LargeDocument] Chunked {filename}: {len(result['chunks'])} chunks, "
                       f"{len(sections)} sections, {result['metadata'].get('pages', 0)} pages")
            
        except Exception as e:
            logger.error(f"Error chunking large document: {e}", exc_info=True)
            # Fallback
            chunks = self._fallback_simple_chunking(text)
            result['chunks'] = [{'text': c, 'section_index': 0} for c in chunks]
            result['metadata']['total_chunks'] = len(chunks)
            result['metadata']['error'] = str(e)
        
        return result
    
    async def _extract_pdf_structure(self, file_content: bytes) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É PDF –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º"""
        pages = []
        try:
            import PyPDF2
            import io
            
            pdf_file = io.BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            for i, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        pages.append({
                            'page_number': i + 1,
                            'text': page_text.strip(),
                            'length': len(page_text)
                        })
                except Exception as e:
                    logger.warning(f"Error extracting page {i+1}: {e}")
                    continue
            
        except Exception as e:
            logger.warning(f"Error extracting PDF structure: {e}")
        
        return pages
    
    def _extract_sections(self, text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–∫—Ü–∏–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        sections = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–µ–∫—Ü–∏–π
        section_patterns = [
            r'^(?:–ì–õ–ê–í–ê|–†–ê–ó–î–ï–õ|–ß–ê–°–¢–¨)\s+\d+[.\s]*(.+)?$',
            r'^(?:\d+\.?\s+)?[A-Z–ê-–Ø–Å][A-Z–ê-–Ø–Å\s]{5,50}$',
            r'^#{1,2}\s+.+$',
        ]
        
        lines = text.split('\n')
        current_section = {'title': '–í–≤–µ–¥–µ–Ω–∏–µ', 'content': '', 'start_line': 0}
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Å–µ–∫—Ü–∏–∏
            is_section_header = False
            for pattern in section_patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE | re.MULTILINE):
                    is_section_header = True
                    break
            
            if is_section_header:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section['content'].strip():
                    sections.append(current_section)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                current_section = {
                    'title': line_stripped,
                    'content': line + '\n',
                    'start_line': i
                }
            else:
                current_section['content'] += line + '\n'
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_section['content'].strip():
            sections.append(current_section)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–µ–∫—Ü–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ –æ–¥–Ω—É —Å–µ–∫—Ü–∏—é
        if not sections:
            sections = [{
                'title': '–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç',
                'content': text,
                'start_line': 0
            }]
        
        return sections


# === –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ü–ê–†–ê–ú–ï–¢–†–ê–ú CHUNKING ===
"""
–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –†–ê–ó–ú–ï–†–£ –ß–ê–ù–ö–û–í:

1. –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–∫—Ç–∞—Ö):
   - chunk_size: 500-800 —Å–∏–º–≤–æ–ª–æ–≤
   - overlap: 100-200 —Å–∏–º–≤–æ–ª–æ–≤
   - top_k: 5-10 —á–∞–Ω–∫–æ–≤

2. –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—é–º–µ:
   - chunk_size: 1500-2000 —Å–∏–º–≤–æ–ª–æ–≤
   - overlap: 300-500 —Å–∏–º–≤–æ–ª–æ–≤
   - top_k: 15-20 —á–∞–Ω–∫–æ–≤

3. –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
   - chunk_size: 2000-3000 —Å–∏–º–≤–æ–ª–æ–≤
   - overlap: 500 —Å–∏–º–≤–æ–ª–æ–≤
   - top_k: 30-50 —á–∞–Ω–∫–æ–≤

–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –°–¢–†–ê–¢–ï–ì–ò–Ø–ú:

1. PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã:
   - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Page-Level ‚Üí Element-Based ‚Üí Recursive
   - Page-Level –ª—É—á—à–µ –¥–ª—è —Ç–∞–±–ª–∏—Ü –∏ —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

2. Word/TXT –¥–æ–∫—É–º–µ–Ω—Ç—ã:
   - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Element-Based ‚Üí Recursive ‚Üí Semantic
   - Element-Based —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏

3. –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (>200 —Å—Ç—Ä–∞–Ω–∏—Ü):
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å chunk_large_document()
   - –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: —Å–µ–∫—Ü–∏–∏ ‚Üí —á–∞–Ω–∫–∏

4. –î–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Page-Level chunking
   - –ù–µ —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
"""

