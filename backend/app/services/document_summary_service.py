"""
–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è summary –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangGraph
"""
import logging
from typing import Optional, List, Dict, Any
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.models.document import Document
from app.llm.openrouter_client import OpenRouterClient
from app.models.project import Project
from app.models.llm_model import GlobalModelSettings
from app.core.config import settings as app_settings
from app.core.prompt_config import get_prompt

logger = logging.getLogger(__name__)


# === –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –ú–û–î–ï–õ–ï–ô –î–õ–Ø SUMMARY ===
"""
–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–ë–™–ï–ú–£ –¢–ï–ö–°–¢–ê:

1. –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ 50 —Å—Ç—Ä–∞–Ω–∏—Ü (~100K —Å–∏–º–≤–æ–ª–æ–≤):
   - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç —Ü–µ–ª–∏–∫–æ–º
   - chunk_size: 1500 —Å–∏–º–≤–æ–ª–æ–≤
   - max_context: 100K —Å–∏–º–≤–æ–ª–æ–≤
   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏: Claude 3.5 Sonnet, GPT-4 Turbo

2. –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 50-200 —Å—Ç—Ä–∞–Ω–∏—Ü (~100-400K —Å–∏–º–≤–æ–ª–æ–≤):
   - –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é "–Ω–∞—á–∞–ª–æ + —Å–µ—Ä–µ–¥–∏–Ω–∞ + –∫–æ–Ω–µ—Ü"
   - –ö–∞–∂–¥–∞—è —á–∞—Å—Ç—å ~30K —Å–∏–º–≤–æ–ª–æ–≤
   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏: Claude 3.5 Sonnet (128K –∫–æ–Ω—Ç–µ–∫—Å—Ç)

3. –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 200+ —Å—Ç—Ä–∞–Ω–∏—Ü (>400K —Å–∏–º–≤–æ–ª–æ–≤):
   - –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ (Map-Reduce)
   - –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–µ–∫—Ü–∏–∏, —Ä–µ–∑—é–º–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é, –æ–±—ä–µ–¥–∏–Ω—è–µ–º
   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏: GPT-4 Turbo, Claude 3.5 Sonnet

–§–û–†–ú–ê–¢ –ü–†–û–ú–ü–¢–û–í –î–õ–Ø –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–• –ò–°–ö–ê–ñ–ï–ù–ò–ô:

1. –¢–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤:
   - "–°–æ—Ö—Ä–∞–Ω–∏ —Ç–æ—á–Ω–æ—Å—Ç—å: —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ"
   - "–ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"

2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
   - "–ù–∞—á–Ω–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Ç–µ–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"
   - "–ü–µ—Ä–µ—á–∏—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã"
   - "–°–¥–µ–ª–∞–π –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"

3. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:
   - temperature: 0.1-0.2 (–Ω–∏–∑–∫–∞—è –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
   - max_tokens: 500-1000 (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ summary)
"""


class DocumentSummaryService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è summary –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, db: AsyncSession):
        self.db = db
    
    async def generate_summary(self, document_id: UUID) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summary –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            Summary –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç (–±–µ–∑–æ–ø–∞—Å–Ω–æ, –¥–∞–∂–µ –µ—Å–ª–∏ –ø–æ–ª–µ summary –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)
            try:
                result = await self.db.execute(
                    select(Document).where(Document.id == document_id)
                )
                document = result.scalar_one_or_none()
            except Exception as db_error:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –ø–æ–ª—è summary, –∏—Å–ø–æ–ª—å–∑—É–µ–º raw SQL
                error_str = str(db_error).lower()
                if "summary" in error_str or "column" in error_str:
                    logger.warning(f"Summary column not found in DB, using raw SQL query")
                    from sqlalchemy import text
                    result = await self.db.execute(
                        text("SELECT id, project_id, filename, content, file_type, created_at FROM documents WHERE id = :doc_id"),
                        {"doc_id": str(document_id)}
                    )
                    row = result.first()
                    if not row:
                        logger.error(f"Document {document_id} not found")
                        return None
                    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Document –≤—Ä—É—á–Ω—É—é
                    document = Document()
                    document.id = row[0]
                    document.project_id = row[1]
                    document.filename = row[2]
                    document.content = row[3] if row[3] else ""
                    document.file_type = row[4]
                    document.created_at = row[5]
                    # –ü–æ–ª–µ summary –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                    try:
                        setattr(document, 'summary', None)
                    except:
                        pass
                else:
                    raise
            
            if not document:
                logger.error(f"Document {document_id} not found")
                return None
            
            # –ï—Å–ª–∏ summary —É–∂–µ –µ—Å—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ)
            doc_summary = getattr(document, 'summary', None)
            if doc_summary and doc_summary.strip():
                logger.info(f"Document {document_id} already has summary")
                return doc_summary
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–µ–∫—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ LLM
            project_result = await self.db.execute(
                select(Project).where(Project.id == document.project_id)
            )
            project = project_result.scalar_one_or_none()
            
            if not project:
                logger.error(f"Project not found for document {document_id}")
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            content = document.content
            content_length = len(content) if content else 0
            
            logger.info(f"[SUMMARY] üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} ({document.filename}):")
            logger.info(f"[SUMMARY]   - Content length: {content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[SUMMARY]   - Content status: {content[:100] if content else 'EMPTY'}...")
            logger.info(f"[SUMMARY]   - Content is '–û–±—Ä–∞–±–æ—Ç–∫–∞...': {content == '–û–±—Ä–∞–±–æ—Ç–∫–∞...'}")
            logger.info(f"[SUMMARY]   - Content is empty: {not content or content == ''}")
            
            if not content or content in ["–û–±—Ä–∞–±–æ—Ç–∫–∞...", "–û–±—Ä–∞–±–æ—Ç–∞–Ω", ""]:
                logger.warning(f"[SUMMARY] ‚ö†Ô∏è Document {document_id} ({document.filename}) has no content yet!")
                logger.warning(f"[SUMMARY] ‚ö†Ô∏è Content value: '{content}'")
                logger.warning(f"[SUMMARY] ‚ö†Ô∏è –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                logger.warning(f"[SUMMARY] ‚ö†Ô∏è Summary –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞)")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                return None
            
            # ‚úÖ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç —Å —É–º–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º
            # –î–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —á–∞—Å—Ç—è–º
            content_length = len(content)
            max_context_length = 100000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (100k —Å–∏–º–≤–æ–ª–æ–≤)
            
            if content_length <= max_context_length:
                # –î–æ–∫—É–º–µ–Ω—Ç –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–∏–∫–æ–º
                content_for_summary = content
                logger.info(f"Document {document_id} fits in one request ({content_length} chars), analyzing full content")
            else:
                # –î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —á–∞—Å—Ç—è–º
                logger.info(f"Document {document_id} is very long ({content_length} chars), using multi-part analysis")
                # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ, —Å–µ—Ä–µ–¥–∏–Ω—É –∏ –∫–æ–Ω–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
                part_size = max_context_length // 3
                beginning = content[:part_size]
                middle_start = content_length // 2 - part_size // 2
                middle = content[middle_start:middle_start + part_size]
                end = content[-part_size:]
                
                content_for_summary = f"""–ù–ê–ß–ê–õ–û –î–û–ö–£–ú–ï–ù–¢–ê:
{beginning}

–°–ï–†–ï–î–ò–ù–ê –î–û–ö–£–ú–ï–ù–¢–ê:
{middle}

–ö–û–ù–ï–¶ –î–û–ö–£–ú–ï–ù–¢–ê:
{end}

–ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç {content_length} —Å–∏–º–≤–æ–ª–æ–≤. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ —Ç—Ä–∏ —á–∞—Å—Ç–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ summary."""
                logger.info(f"Using multi-part analysis: beginning ({len(beginning)}), middle ({len(middle)}), end ({len(end)})")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å LLM
            primary_model = None
            fallback_model = None
            
            if project.llm_model:
                primary_model = project.llm_model
            else:
                settings_result = await self.db.execute(select(GlobalModelSettings).limit(1))
                global_settings = settings_result.scalar_one_or_none()
                if global_settings:
                    primary_model = global_settings.primary_model_id
                    fallback_model = global_settings.fallback_model_id
            
            if not primary_model:
                primary_model = app_settings.OPENROUTER_MODEL_PRIMARY
            if not fallback_model:
                fallback_model = app_settings.OPENROUTER_MODEL_FALLBACK
            
            # –°–æ–∑–¥–∞–µ–º LLM –∫–ª–∏–µ–Ω—Ç
            llm_client = OpenRouterClient(
                model_primary=primary_model,
                model_fallback=fallback_model
            )
            
            # ‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è summary —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º–∏
            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–æ–∑–¥–∞–π —Ç–æ—á–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (summary) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {document.filename}
–¢–∏–ø —Ñ–∞–π–ª–∞: {document.file_type}
–û–±—â–∞—è –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {content_length} —Å–∏–º–≤–æ–ª–æ–≤

–°–û–î–ï–†–ñ–ò–ú–û–ï –î–û–ö–£–ú–ï–ù–¢–ê:
{content_for_summary}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö SUMMARY:
1. –Ø–∑—ã–∫: –¢–û–õ–¨–ö–û —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
2. –î–ª–∏–Ω–∞: 300-600 —Å–∏–º–≤–æ–ª–æ–≤ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è)
3. –¢–æ—á–Ω–æ—Å—Ç—å: –û—Ç—Ä–∞–∑–∏ –í–°–ï –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
4. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è: –°–æ—Ö—Ä–∞–Ω–∏ —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤, —Ü–∏—Ñ—Ä, –¥–∞—Ç, –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö
5. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –ù–∞—á–Ω–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Ç–µ–º—ã, –∑–∞—Ç–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã
6. –ü–æ–ª–Ω–æ—Ç–∞: –£–ø–æ–º—è–Ω–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
7. –§–æ—Ä–º–∞—Ç: –°–ø–ª–æ—à–Ω–æ–π —Ç–µ–∫—Å—Ç –±–µ–∑ –º–∞—Ä–∫–µ—Ä–æ–≤, –Ω—É–º–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
8. –°—Ç–∏–ª—å: –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤

–í–ê–ñ–ù–û: 
- –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö —Ç–æ—á–Ω–æ
- –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∞–∂–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏–ª–∏ –¥–∞—Ç—ã, –≤–∫–ª—é—á–∏ –∏—Ö –≤ summary
- –°–æ—Ö—Ä–∞–Ω–∏ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ

–°–æ–∑–¥–∞–π —Ç–æ–ª—å–∫–æ summary, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ –ø—Ä–µ–¥–∏—Å–ª–æ–≤–∏–π:"""
            
            messages = [
                {
                    "role": "system",
                    "content": get_prompt("prompts.system.summary_generator")
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            logger.info(f"Generating summary for document {document_id} ({document.filename}), content length: {content_length}")
            # ‚úÖ –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º max_tokens –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ summary (300->500)
            # ‚úÖ –ù–∏–∑–∫–∞—è temperature (0.2) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∏—Å–∫–∞–∂–µ–Ω–∏–π
            summary = await llm_client.chat_completion(
                messages=messages,
                max_tokens=500,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ summary
                temperature=0.2  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            )
            
            # –û—á–∏—â–∞–µ–º summary –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            summary = summary.strip()
            if summary.startswith("Summary:") or summary.startswith("–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:"):
                summary = summary.split(":", 1)[1].strip()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º summary –≤ –ë–î (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ–ª–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
            if hasattr(document, 'summary'):
                document.summary = summary
                await self.db.commit()
                await self.db.refresh(document)
            else:
                logger.warning(f"Summary field does not exist in database, cannot save summary for document {document_id}")
            
            logger.info(f"Summary generated for document {document_id}, length: {len(summary)}")
            return summary
            
        except Exception as e:
            logger.error(f"Error generating summary for document {document_id}: {e}", exc_info=True)
            return None
    
    async def generate_summaries_for_project(self, project_id: UUID) -> int:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summaries –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ –±–µ–∑ summary
        
        Args:
            project_id: ID –ø—Ä–æ–µ–∫—Ç–∞
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö summaries
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞ –±–µ–∑ summary
            result = await self.db.execute(
                select(Document)
                .where(Document.project_id == project_id)
                .where((Document.summary == None) | (Document.summary == ""))
            )
            documents = result.scalars().all()
            
            count = 0
            for doc in documents:
                summary = await self.generate_summary(doc.id)
                if summary:
                    count += 1
            
            logger.info(f"Generated {count} summaries for project {project_id}")
            return count
            
        except Exception as e:
            logger.error(f"Error generating summaries for project {project_id}: {e}", exc_info=True)
            return 0
    
    async def generate_summary_with_langgraph(self, document_id: UUID) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summary –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangGraph workflow
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            Summary –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            from app.services.langgraph_rag_workflow import (
                LangGraphRAGWorkflow, 
                QueryType,
                RAGConfig
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            result = await self.db.execute(
                select(Document).where(Document.id == document_id)
            )
            document = result.scalar_one_or_none()
            
            if not document:
                logger.error(f"Document {document_id} not found")
                return None
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è summary —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏—Å–∫–∞–∂–µ–Ω–∏—è–º–∏
            config = RAGConfig(
                max_context_tokens=100000,
                max_output_tokens=1000,
                chunk_size=2000,
                chunk_overlap=400,
                top_k_retrieval=20,
                temperature=0.1  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            )
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º LangGraph workflow
            rag_workflow = LangGraphRAGWorkflow(self.db, config)
            result = await rag_workflow.run(
                query=f"–°–æ–∑–¥–∞–π —Ç–æ—á–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.filename}",
                query_type=QueryType.SUMMARY,
                project_id=str(document.project_id),
                document_id=str(document_id)
            )
            
            summary = result.get('answer', '')
            
            if summary:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º summary –≤ –ë–î
                if hasattr(document, 'summary'):
                    document.summary = summary
                    await self.db.commit()
                    await self.db.refresh(document)
                
                logger.info(f"LangGraph summary generated for document {document_id}, length: {len(summary)}")
                return summary
            
            return None
            
        except Exception as e:
            logger.error(f"Error generating LangGraph summary for document {document_id}: {e}", exc_info=True)
            return None
    
    async def generate_map_reduce_summary(
        self, 
        document_id: UUID,
        max_chunk_size: int = 30000
    ) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summary –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –º–µ—Ç–æ–¥–æ–º Map-Reduce
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        1. –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –±–æ–ª—å—à–∏–µ —Å–µ–∫—Ü–∏–∏
        2. –°–æ–∑–¥–∞–µ–º summary –¥–ª—è –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏ (Map)
        3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ summaries –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ (Reduce)
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            max_chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–µ–∫—Ü–∏–∏ (—Å–∏–º–≤–æ–ª–æ–≤)
        
        Returns:
            Summary –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            result = await self.db.execute(
                select(Document).where(Document.id == document_id)
            )
            document = result.scalar_one_or_none()
            
            if not document or not document.content:
                logger.error(f"Document {document_id} not found or empty")
                return None
            
            content = document.content
            content_length = len(content)
            
            logger.info(f"[Map-Reduce] Starting for document {document_id}, length: {content_length}")
            
            # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ–±–æ–ª—å—à–æ–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
            if content_length <= max_chunk_size:
                return await self.generate_summary(document_id)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–µ–∫—Ü–∏–∏
            sections = []
            for i in range(0, content_length, max_chunk_size):
                section = content[i:i + max_chunk_size]
                sections.append(section)
            
            logger.info(f"[Map-Reduce] Document split into {len(sections)} sections")
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM
            primary_model, fallback_model = await self._get_llm_models(document.project_id)
            llm_client = OpenRouterClient(
                model_primary=primary_model,
                model_fallback=fallback_model
            )
            
            # Map: –°–æ–∑–¥–∞–µ–º summary –¥–ª—è –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏
            section_summaries = []
            for i, section in enumerate(sections):
                logger.info(f"[Map-Reduce] Processing section {i+1}/{len(sections)}")
                
                map_prompt = f"""–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–ª–µ–¥—É—é—â–µ–π —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ({i+1}/{len(sections)}):

–°–û–î–ï–†–ñ–ò–ú–û–ï:
{section}

–†–ï–ó–Æ–ú–ï –ß–ê–°–¢–ò (100-200 —Å–ª–æ–≤):"""
                
                messages = [
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ. –°–æ—Ö—Ä–∞–Ω—è–π –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã."},
                    {"role": "user", "content": map_prompt}
                ]
                
                try:
                    section_summary = await llm_client.chat_completion(
                        messages=messages,
                        max_tokens=400,
                        temperature=0.1
                    )
                    section_summaries.append(f"–ß–∞—Å—Ç—å {i+1}: {section_summary.strip()}")
                except Exception as e:
                    logger.warning(f"[Map-Reduce] Error summarizing section {i+1}: {e}")
                    continue
            
            if not section_summaries:
                logger.error("[Map-Reduce] No section summaries generated")
                return None
            
            # Reduce: –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ summary
            combined_summaries = "\n\n".join(section_summaries)
            
            reduce_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—é–º–µ —á–∞—Å—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞ "{document.filename}" —Å–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–µ–∑—é–º–µ.

–†–ï–ó–Æ–ú–ï –ß–ê–°–¢–ï–ô:
{combined_summaries}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –ò–¢–û–ì–û–í–û–ú–£ –†–ï–ó–Æ–ú–ï:
1. –î–ª–∏–Ω–∞: 500-1000 —Å–∏–º–≤–æ–ª–æ–≤
2. –í–∫–ª—é—á–∏ –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –∏–∑ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
3. –°–æ—Ö—Ä–∞–Ω–∏ —Ç–æ—á–Ω–æ—Å—Ç—å: —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, –∏–º–µ–Ω–∞
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≥–ª–∞–≤–Ω–∞—è —Ç–µ–º–∞ ‚Üí –∫–ª—é—á–µ–≤—ã–µ –ø—É–Ω–∫—Ç—ã ‚Üí –≤—ã–≤–æ–¥—ã
5. –Ø–∑—ã–∫: —Ä—É—Å—Å–∫–∏–π

–ò–¢–û–ì–û–í–û–ï –†–ï–ó–Æ–ú–ï:"""
            
            messages = [
                {"role": "system", "content": get_prompt("prompts.system.summary_generator")},
                {"role": "user", "content": reduce_prompt}
            ]
            
            final_summary = await llm_client.chat_completion(
                messages=messages,
                max_tokens=800,
                temperature=0.1
            )
            
            final_summary = final_summary.strip()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            if hasattr(document, 'summary'):
                document.summary = final_summary
                await self.db.commit()
                await self.db.refresh(document)
            
            logger.info(f"[Map-Reduce] Final summary generated, length: {len(final_summary)}")
            return final_summary
            
        except Exception as e:
            logger.error(f"[Map-Reduce] Error: {e}", exc_info=True)
            return None
    
    async def _get_llm_models(self, project_id: UUID) -> tuple:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞"""
        primary_model = None
        fallback_model = None
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–µ–∫—Ç
            project_result = await self.db.execute(
                select(Project).where(Project.id == project_id)
            )
            project = project_result.scalar_one_or_none()
            
            if project and project.llm_model:
                primary_model = project.llm_model
            else:
                # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                settings_result = await self.db.execute(select(GlobalModelSettings).limit(1))
                global_settings = settings_result.scalar_one_or_none()
                if global_settings:
                    primary_model = global_settings.primary_model_id
                    fallback_model = global_settings.fallback_model_id
        except Exception as e:
            logger.warning(f"Error getting LLM models: {e}")
        
        if not primary_model:
            primary_model = app_settings.OPENROUTER_MODEL_PRIMARY
        if not fallback_model:
            fallback_model = app_settings.OPENROUTER_MODEL_FALLBACK
        
        return primary_model, fallback_model
    
    async def describe_document_content(self, document_id: UUID) -> Optional[str]:
        """
        –°–æ–∑–¥–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            from app.services.langgraph_rag_workflow import (
                LangGraphRAGWorkflow, 
                QueryType
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            result = await self.db.execute(
                select(Document).where(Document.id == document_id)
            )
            document = result.scalar_one_or_none()
            
            if not document:
                return None
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º LangGraph workflow
            rag_workflow = LangGraphRAGWorkflow(self.db)
            result = await rag_workflow.run(
                query=f"–û–ø–∏—à–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.filename}",
                query_type=QueryType.DESCRIPTION,
                project_id=str(document.project_id),
                document_id=str(document_id)
            )
            
            return result.get('answer', '')
            
        except Exception as e:
            logger.error(f"Error describing document {document_id}: {e}", exc_info=True)
            return None
    
    async def analyze_document(self, document_id: UUID) -> Optional[Dict[str, Any]]:
        """
        –ü—Ä–æ–≤–æ–¥–∏—Ç –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∞–Ω–∞–ª–∏–∑–æ–º: —Ç–∏–ø, —Ç–µ–º—ã, –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        """
        try:
            from app.services.langgraph_rag_workflow import (
                LangGraphRAGWorkflow, 
                QueryType
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            result = await self.db.execute(
                select(Document).where(Document.id == document_id)
            )
            document = result.scalar_one_or_none()
            
            if not document:
                return None
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º LangGraph workflow
            rag_workflow = LangGraphRAGWorkflow(self.db)
            result = await rag_workflow.run(
                query=f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç {document.filename}",
                query_type=QueryType.ANALYSIS,
                project_id=str(document.project_id),
                document_id=str(document_id)
            )
            
            return {
                'document_id': str(document_id),
                'filename': document.filename,
                'analysis': result.get('answer', ''),
                'sources': result.get('sources', []),
                'metadata': result.get('metadata', {})
            }
            
        except Exception as e:
            logger.error(f"Error analyzing document {document_id}: {e}", exc_info=True)
            return None

