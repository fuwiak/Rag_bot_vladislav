"""
Celery –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–≥—Ä—É–∑–∫—É –≤ PostgreSQL –∏ Qdrant
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""
import os
import gc
import logging
from uuid import UUID
from celery import Task
from app.celery_app import celery_app
from app.core.database import AsyncSessionLocal

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
LARGE_DOCUMENT_THRESHOLD = 500_000  # 500KB —Ç–µ–∫—Å—Ç–∞
VERY_LARGE_DOCUMENT_THRESHOLD = 2_000_000  # 2MB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 200+ —Å—Ç—Ä–∞–Ω–∏—Ü)
MAX_BATCH_SIZE_LARGE = 5  # –ú–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
MAX_BATCH_SIZE_NORMAL = 10  # –û–±—ã—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
MAX_BATCH_SIZE_VERY_LARGE = 3  # –ï—â–µ –º–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
PDF_PAGES_PER_BATCH = 50  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF –ø–æ 50 —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑


class DatabaseTask(Task):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –ë–î"""
    _db = None

    @property
    def db(self):
        if self._db is None:
            # –î–ª—è Celery –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –ë–î
            # –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
            pass
        return self._db


@celery_app.task(bind=True, name='app.tasks.document_tasks.process_document_task')
def process_document_task(self, document_id: str, project_id: str, file_path: str, filename: str, file_type: str):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
    –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –≤–æ—Ä–∫–µ—Ä–µ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è out of memory
    """
    import asyncio
    import psutil
    
    process = psutil.Process(os.getpid())
    start_memory = process.memory_info().rss / 1024 / 1024
    logger.info(f"[Celery] Starting processing document {document_id} ({filename}), memory: {start_memory:.2f}MB")
    
    file_content = None
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if not os.path.exists(file_path):
            logger.error(f"[Celery] File not found: {file_path}")
            return {"status": "error", "message": f"File not found: {file_path}"}
        
        file_size = os.path.getsize(file_path) / 1024 / 1024
        logger.info(f"[Celery] Reading file {file_path}, size: {file_size:.2f}MB")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        read_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Celery] File read into memory, memory: {read_memory:.2f}MB (delta: {read_memory - start_memory:.2f}MB)")
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —á—Ç–µ–Ω–∏—è
        try:
            os.unlink(file_path)
            logger.info(f"[Celery] Temp file deleted: {file_path}")
        except Exception as e:
            logger.warning(f"[Celery] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {file_path}: {e}")
        
        # –í—ã–∑—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        # Celery –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.run –¥–ª—è async —Ñ—É–Ω–∫—Ü–∏–π
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                process_document_async(
                    UUID(document_id),
                    UUID(project_id),
                    file_content,
                    filename,
                    file_type
                )
            )
            logger.info(f"[Celery] Document {document_id} processed successfully")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"[Celery] Error processing document {document_id}: {e}", exc_info=True)
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if file_path and os.path.exists(file_path):
            try:
                os.unlink(file_path)
            except:
                pass
        return {"status": "error", "message": str(e)}
    finally:
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
        if file_content is not None:
            del file_content
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Celery] Processing complete for document {document_id}, final memory: {final_memory:.2f}MB")


async def process_document_async(document_id: UUID, project_id: UUID, file_content: bytes, filename: str, file_type: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–∞—Ä—Å–∏–Ω–≥, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant)"""
    import gc
    import psutil
    
    process = psutil.Process(os.getpid())
    try:
        async with AsyncSessionLocal() as db:
            # –ü–∞—Ä—Å–∏–Ω–≥ –∏ —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            from app.documents.parser import DocumentParser
            from app.documents.chunker import DocumentChunker
            from app.models.document import Document
            from sqlalchemy import select
            
            parser = DocumentParser()
            chunker = DocumentChunker()
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ü–ï–†–ï–î –ø–∞—Ä—Å–∏–Ω–≥–æ–º, aby m√≥c szybko zaktualizowaƒá content
            result = await db.execute(select(Document).where(Document.id == document_id))
            document = result.scalar_one_or_none()
            if not document:
                logger.error(f"[Celery] Document {document_id} not found")
                return
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            try:
                text = await parser.parse(file_content, file_type)
            except Exception as e:
                logger.error(f"[Celery] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} ({filename}): {e}")
                document.content = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:200]}"
                await db.commit()
                return
            
            # –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º content –ù–ï–ú–ï–î–õ–ï–ù–ù–û –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á—Ç–æ–±—ã RAG –º–æ–≥ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞: 2MB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 2,000,000 —Å–∏–º–≤–æ–ª–æ–≤)
            logger.info(f"[Celery] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ RAG...")
            MAX_CONTENT_SIZE = 2_000_000
            if len(text) > MAX_CONTENT_SIZE:
                logger.warning(f"[Celery] ‚ö†Ô∏è Document {document_id} content —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {MAX_CONTENT_SIZE}")
                document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
            else:
                document.content = text
            
            # –ö–û–ú–ú–ò–¢–ò–ú –°–†–ê–ó–£, —á—Ç–æ–±—ã content by≈Ç dostƒôp–µ–Ω –¥–ª—è RAG
            await db.commit()
            await db.refresh(document)
            
            logger.info(f"[Celery] ‚úÖ‚úÖ‚úÖ –î–û–ö–£–ú–ï–ù–¢ –ì–û–¢–û–í –î–õ–Ø RAG –ó–ê–ü–†–û–°–û–í ‚úÖ‚úÖ‚úÖ")
            logger.info(f"[Celery] üìÑ Document ID: {document_id}")
            logger.info(f"[Celery] üìÑ Filename: {filename}")
            logger.info(f"[Celery] üìÑ Text length: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery] üìÑ Content saved: {len(document.content)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery] üìÑ Document is now READY for RAG queries - –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã!")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if document.content:
                preview = document.content[:500] if len(document.content) > 500 else document.content
                logger.info(f"[Celery] üìÑ Content preview (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {preview}...")
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunker —Å fallback-–∞–º–∏
            logger.info(f"[Celery] üî™ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–±–∏–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏: {filename}, —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            from app.documents.advanced_chunker import AdvancedChunker
            advanced_chunker = AdvancedChunker(
                default_chunk_size=800,
                default_overlap=200,
                min_chunk_size=100,
                max_chunk_size=2000
            )
            
            chunking_start_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Celery] üî™ –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ chunking: {chunking_start_memory:.2f}MB")
            
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunking
            chunks = await advanced_chunker.chunk_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename
            )
            
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π chunker –µ—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            if not chunks or len(chunks) == 0:
                logger.warning(f"[Celery] ‚ö†Ô∏è Advanced chunking failed, using simple chunker")
                chunks = chunker.chunk_text(text)
            if not chunks:
                logger.warning(f"[Celery] ‚ùå –î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ chunking")
                return
            
            chunking_end_memory = process.memory_info().rss / 1024 / 1024
            total_chunks = len(chunks)
            avg_chunk_size = sum(len(c) for c in chunks) / total_chunks if chunks else 0
            logger.info(f"[Celery] ‚úÖ Document split into {total_chunks} chunks:")
            logger.info(f"[Celery]   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery]   - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(len(c) for c in chunks) if chunks else 0} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery]   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(len(c) for c in chunks) if chunks else 0} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery]   - –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ chunking: {chunking_end_memory:.2f}MB (delta: {chunking_end_memory - chunking_start_memory:.2f}MB)")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            from app.services.embedding_service import EmbeddingService
            from app.vector_db.vector_store import VectorStore
            from app.models.document import DocumentChunk
            
            embedding_service = EmbeddingService()
            vector_store = VectorStore()
            
            # Batch processing –¥–ª—è szybszego zapisu do Qdrant
            BATCH_SIZE = 10  # Zapisujemy po 10 chunks naraz
            batch_points = []
            batch_chunks = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            logger.info(f"[Celery] üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(chunks)} —á–∞–Ω–∫–æ–≤: —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant")
            embedding_start_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Celery] üìä –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embedding_start_memory:.2f}MB")
            
            successful_chunks = 0
            failed_chunks = 0
            
            for chunk_index, chunk_text in enumerate(chunks):
                try:
                    chunk_memory_before = process.memory_info().rss / 1024 / 1024
                    
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
                    try:
                        logger.debug(f"[Celery] üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {chunk_index + 1}/{len(chunks)}: —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ ({len(chunk_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                        embedding = await embedding_service.create_embedding(chunk_text)
                        logger.debug(f"[Celery] ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index + 1}, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(embedding)}")
                    except Exception as e:
                        logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index + 1}: {e}")
                        failed_chunks += 1
                        continue
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ –≤ –ë–î (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞)
                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 10KB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 10,000 —Å–∏–º–≤–æ–ª–æ–≤)
                    MAX_CHUNK_SIZE = 10_000
                    chunk_text_to_save = chunk_text[:MAX_CHUNK_SIZE] if len(chunk_text) > MAX_CHUNK_SIZE else chunk_text
                    if len(chunk_text) > MAX_CHUNK_SIZE:
                        logger.warning(f"[Celery] ‚ö†Ô∏è Chunk {chunk_index + 1} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({len(chunk_text)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {MAX_CHUNK_SIZE}")
                    
                    chunk = DocumentChunk(
                        document_id=document_id,
                        chunk_text=chunk_text_to_save,
                        chunk_index=chunk_index
                    )
                    db.add(chunk)
                    await db.flush()  # –ü–æ–ª—É—á–∞–µ–º ID —á–∞–Ω–∫–∞
                    
                    # Dodajemy do batcha dla szybszego zapisu do Qdrant
                    from qdrant_client.models import PointStruct
                    point_id = chunk.id  # U≈ºywamy chunk.id jako point_id
                    batch_points.append(PointStruct(
                        id=str(point_id),
                        vector=embedding,
                        payload={
                            "document_id": str(document_id),
                            "chunk_id": str(chunk.id),
                            "chunk_index": chunk_index,
                            "filename": filename,
                            "chunk_text": chunk_text[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è Qdrant
                        }
                    ))
                    batch_chunks.append((chunk, point_id))
                    
                    # Zapisujemy batch co BATCH_SIZE chunks
                    if len(batch_points) >= BATCH_SIZE or chunk_index == len(chunks) - 1:
                        try:
                            # Batch upsert do Qdrant
                            collection_name = f"project_{project_id}"
                            logger.info(f"[Celery] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–∞ –∏–∑ {len(batch_points)} —á–∞–Ω–∫–æ–≤ –≤ Qdrant (–∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection_name})")
                            await vector_store.ensure_collection(collection_name, len(embedding))
                            vector_store.client.upsert(
                                collection_name=collection_name,
                                points=batch_points
                            )
                            
                            # Aktualizujemy qdrant_point_id dla wszystkich chunks w batchu
                            for batch_chunk, batch_point_id in batch_chunks:
                                batch_chunk.qdrant_point_id = batch_point_id
                            await db.flush()
                            
                            successful_chunks += len(batch_points)
                            progress_pct = ((chunk_index + 1) / len(chunks)) * 100
                            logger.info(f"[Celery] ‚úÖ –ë–∞—Ç—á –∏–∑ {len(batch_points)} —á–∞–Ω–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ Qdrant (–ø—Ä–æ–≥—Ä–µ—Å—Å: {chunk_index + 1}/{len(chunks)} = {progress_pct:.1f}%)")
                        except Exception as e:
                            logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ batch upsert –≤ Qdrant: {e}", exc_info=True)
                            failed_chunks += len(batch_points)
                            # Pr√≥bujemy zapisaƒá pojedynczo jako fallback
                            for batch_chunk, batch_point_id in batch_chunks:
                                try:
                                    await vector_store.store_vector(
                                        collection_name=f"project_{project_id}",
                                        vector=embedding,  # U≈ºywamy ostatniego embedding
                                        payload={
                                            "document_id": str(document_id),
                                            "chunk_id": str(batch_chunk.id),
                                            "chunk_index": batch_chunk.chunk_index,
                                            "filename": filename,
                                            "chunk_text": batch_chunk.chunk_text[:500]
                                        }
                                    )
                                    batch_chunk.qdrant_point_id = batch_point_id
                                    successful_chunks += 1
                                    failed_chunks -= 1
                                except Exception as fallback_error:
                                    logger.error(f"[Celery] ‚ùå Fallback —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–∞ {batch_chunk.chunk_index} —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {fallback_error}")
                        
                        # Czyszczenie batcha
                        batch_points = []
                        batch_chunks = []
                    
                    chunk_memory_after = process.memory_info().rss / 1024 / 1024
                    if (chunk_index + 1) % 10 == 0:
                        progress_pct = ((chunk_index + 1) / len(chunks)) * 100
                        logger.info(f"[Celery] üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {chunk_index + 1}/{len(chunks)} —á–∞–Ω–∫–æ–≤ ({progress_pct:.1f}%), –ø–∞–º—è—Ç—å: {chunk_memory_after:.2f}MB, —É—Å–ø–µ—à–Ω–æ: {successful_chunks}, –æ—à–∏–±–æ–∫: {failed_chunks}")
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤
                    if (chunk_index + 1) % 10 == 0:
                        gc.collect()
                    
                except Exception as e:
                    logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {chunk_index + 1}: {e}", exc_info=True)
                    failed_chunks += 1
                    continue
            
            embedding_end_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Celery] ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
            logger.info(f"[Celery]   - –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            logger.info(f"[Celery]   - –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_chunks}")
            logger.info(f"[Celery]   - –û—à–∏–±–æ–∫: {failed_chunks}")
            logger.info(f"[Celery]   - –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {embedding_end_memory:.2f}MB (delta: {embedding_end_memory - embedding_start_memory:.2f}MB)")
            
            # –ö–æ–º–º–∏—Ç–∏–º –≤—Å–µ —á–∞–Ω–∫–∏
            await db.commit()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            result = await db.execute(select(Document).where(Document.id == document_id))
            document = result.scalar_one_or_none()
            if document:
                content_length = len(document.content) if document.content else 0
                logger.info(f"[Celery] ‚úÖ Document {document_id} processed successfully:")
                logger.info(f"  - Filename: {filename}")
                logger.info(f"  - Chunks created: {len(chunks)}")
                logger.info(f"  - Content length: {content_length} chars")
                logger.info(f"  - Content status: {'READY' if content_length > 100 else 'EMPTY'}")
                if document.content and content_length > 0:
                    preview = document.content[:300] if content_length > 300 else document.content
                    logger.info(f"  - Content preview: {preview}...")
            else:
                logger.error(f"[Celery] ‚ùå Document {document_id} not found after processing!")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM (–≤ —Ñ–æ–Ω–µ)
            try:
                from app.services.document_summary_service import DocumentSummaryService
                summary_service = DocumentSummaryService(db)
                summary = await summary_service.generate_summary(document_id)
                if summary:
                    logger.info(f"[Celery] Summary generated for document {document_id}")
            except Exception as summary_error:
                logger.warning(f"[Celery] Error generating summary for document {document_id}: {summary_error}")
            
    except Exception as e:
        logger.error(f"[Celery] Error processing document {document_id}: {e}", exc_info=True)
    finally:
        gc.collect()


@celery_app.task(bind=True, name='app.tasks.document_tasks.generate_document_summary_task')
def generate_document_summary_task(self, document_id: str):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ summary –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM
    """
    import asyncio
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async def generate_summary_async():
                async with AsyncSessionLocal() as db:
                    from app.services.document_summary_service import DocumentSummaryService
                    summary_service = DocumentSummaryService(db)
                    summary = await summary_service.generate_summary(UUID(document_id))
                    return summary
            
            result = loop.run_until_complete(generate_summary_async())
            logger.info(f"[Celery] Summary generated for document {document_id}")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"[Celery] Error generating summary for document {document_id}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


@celery_app.task(bind=True, name='app.tasks.document_tasks.process_large_document_with_langgraph')
def process_large_document_with_langgraph(
    self, 
    document_id: str, 
    project_id: str, 
    file_path: str, 
    filename: str, 
    file_type: str
):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangGraph
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è PDF >100 —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    import asyncio
    import psutil
    
    process = psutil.Process(os.getpid())
    start_memory = process.memory_info().rss / 1024 / 1024
    logger.info(f"[Celery LangGraph] Starting processing document {document_id} ({filename}), memory: {start_memory:.2f}MB")
    
    file_content = None
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if not os.path.exists(file_path):
            logger.error(f"[Celery LangGraph] File not found: {file_path}")
            return {"status": "error", "message": f"File not found: {file_path}"}
        
        file_size = os.path.getsize(file_path) / 1024 / 1024
        logger.info(f"[Celery LangGraph] Reading file {file_path}, size: {file_size:.2f}MB")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        try:
            os.unlink(file_path)
            logger.info(f"[Celery LangGraph] Temp file deleted: {file_path}")
        except Exception as e:
            logger.warning(f"[Celery LangGraph] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {file_path}: {e}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                process_large_document_async_langgraph(
                    UUID(document_id),
                    UUID(project_id),
                    file_content,
                    filename,
                    file_type
                )
            )
            logger.info(f"[Celery LangGraph] Document {document_id} processed successfully")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"[Celery LangGraph] Error processing document {document_id}: {e}", exc_info=True)
        if file_path and os.path.exists(file_path):
            try:
                os.unlink(file_path)
            except:
                pass
        return {"status": "error", "message": str(e)}
    finally:
        if file_content is not None:
            del file_content
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Celery LangGraph] Processing complete, final memory: {final_memory:.2f}MB")


async def process_large_document_async_langgraph(
    document_id: UUID, 
    project_id: UUID, 
    file_content: bytes, 
    filename: str, 
    file_type: str
):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å LangGraph
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö PDF
    """
    import gc
    import psutil
    
    process = psutil.Process(os.getpid())
    
    async with AsyncSessionLocal() as db:
        from app.documents.parser import DocumentParser
        from app.documents.advanced_chunker import AdvancedChunker
        from app.models.document import Document, DocumentChunk
        from sqlalchemy import select
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        result = await db.execute(select(Document).where(Document.id == document_id))
        document = result.scalar_one_or_none()
        if not document:
            logger.error(f"[Celery LangGraph] Document {document_id} not found")
            return
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        parser = DocumentParser()
        try:
            text = await parser.parse(file_content, file_type)
        except Exception as e:
            logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            document.content = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:200]}"
            await db.commit()
            return
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
        is_large_document = len(text) > LARGE_DOCUMENT_THRESHOLD
        is_very_large_document = len(text) > VERY_LARGE_DOCUMENT_THRESHOLD
        logger.info(f"[Celery LangGraph] Document size: {len(text)} chars, is_large: {is_large_document}, is_very_large: {is_very_large_document}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        MAX_CONTENT_SIZE = 2_000_000
        if len(text) > MAX_CONTENT_SIZE:
            document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
        else:
            document.content = text
        
        await db.commit()
        await db.refresh(document)
        logger.info(f"[Celery LangGraph] ‚úÖ Document content saved: {len(document.content)} chars")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º advanced chunker —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        advanced_chunker = AdvancedChunker(
            default_chunk_size=1500 if is_large_document else 800,
            default_overlap=300 if is_large_document else 200,
            min_chunk_size=100,
            max_chunk_size=3000 if is_large_document else 2000
        )
        
        if is_large_document:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
            chunk_result = await advanced_chunker.chunk_large_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename,
                use_hierarchical=True
            )
            chunks = [c['text'] for c in chunk_result.get('chunks', [])]
            sections = chunk_result.get('sections', [])
            logger.info(f"[Celery LangGraph] Hierarchical chunking: {len(chunks)} chunks, {len(sections)} sections")
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π chunking
            chunks = await advanced_chunker.chunk_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename
            )
        
        if not chunks:
            logger.warning(f"[Celery LangGraph] No chunks generated for document {document_id}")
            return
        
        logger.info(f"[Celery LangGraph] Document split into {len(chunks)} chunks")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant
        from app.services.embedding_service import EmbeddingService
        from app.vector_db.vector_store import VectorStore
        from qdrant_client.models import PointStruct
        
        embedding_service = EmbeddingService()
        vector_store = VectorStore()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if is_very_large_document:
            batch_size = MAX_BATCH_SIZE_VERY_LARGE
            logger.info(f"[Celery LangGraph] –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á ({batch_size}) –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        elif is_large_document:
            batch_size = MAX_BATCH_SIZE_LARGE
        else:
            batch_size = MAX_BATCH_SIZE_NORMAL
        
        batch_points = []
        batch_chunks = []
        
        for chunk_index, chunk_text in enumerate(chunks):
            try:
                chunk_memory_before = process.memory_info().rss / 1024 / 1024
                
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                try:
                    embedding = await embedding_service.create_embedding(chunk_text)
                except Exception as e:
                    logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index}: {e}")
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ –≤ –ë–î
                MAX_CHUNK_SIZE = 10_000
                chunk_text_to_save = chunk_text[:MAX_CHUNK_SIZE] if len(chunk_text) > MAX_CHUNK_SIZE else chunk_text
                
                chunk = DocumentChunk(
                    document_id=document_id,
                    chunk_text=chunk_text_to_save,
                    chunk_index=chunk_index
                )
                db.add(chunk)
                await db.flush()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á –¥–ª—è Qdrant
                point_id = chunk.id
                batch_points.append(PointStruct(
                    id=str(point_id),
                    vector=embedding,
                    payload={
                        "document_id": str(document_id),
                        "chunk_id": str(chunk.id),
                        "chunk_index": chunk_index,
                        "filename": filename,
                        "chunk_text": chunk_text[:500]
                    }
                ))
                batch_chunks.append((chunk, point_id))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á
                if len(batch_points) >= batch_size or chunk_index == len(chunks) - 1:
                    try:
                        collection_name = f"project_{project_id}"
                        await vector_store.ensure_collection(collection_name, len(embedding))
                        vector_store.client.upsert(
                            collection_name=collection_name,
                            points=batch_points
                        )
                        
                        for batch_chunk, batch_point_id in batch_chunks:
                            batch_chunk.qdrant_point_id = batch_point_id
                        await db.flush()
                        
                        logger.info(f"[Celery LangGraph] ‚úÖ Batch upserted {len(batch_points)} chunks (up to {chunk_index})")
                    except Exception as e:
                        logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ batch upsert –≤ Qdrant: {e}")
                    
                    batch_points = []
                    batch_chunks = []
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (—á–∞—â–µ –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
                log_interval = 10 if is_very_large_document else 20
                if chunk_index % log_interval == 0:
                    chunk_memory_after = process.memory_info().rss / 1024 / 1024
                    progress_pct = (chunk_index + 1) / len(chunks) * 100
                    logger.info(f"[Celery LangGraph] Processed {chunk_index + 1}/{len(chunks)} ({progress_pct:.1f}%), memory: {chunk_memory_after:.2f}MB")
                    gc.collect()
                
            except Exception as e:
                logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {chunk_index}: {e}", exc_info=True)
                continue
        
        # –ö–æ–º–º–∏—Ç–∏–º –≤—Å–µ —á–∞–Ω–∫–∏
        await db.commit()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangGraph
        try:
            from app.services.document_summary_service import DocumentSummaryService
            summary_service = DocumentSummaryService(db)
            
            if is_large_document:
                # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º Map-Reduce
                summary = await summary_service.generate_map_reduce_summary(document_id)
            else:
                # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º LangGraph
                summary = await summary_service.generate_summary_with_langgraph(document_id)
            
            if summary:
                logger.info(f"[Celery LangGraph] Summary generated for document {document_id}")
        except Exception as summary_error:
            logger.warning(f"[Celery LangGraph] Error generating summary: {summary_error}")
        
        logger.info(f"[Celery LangGraph] ‚úÖ Document {document_id} processing complete: {len(chunks)} chunks")


@celery_app.task(bind=True, name='app.tasks.document_tasks.reindex_document_to_qdrant')
def reindex_document_to_qdrant(self, document_id: str, project_id: str):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant
    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ PostgreSQL –≤ Qdrant
    """
    import asyncio
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                reindex_document_async(UUID(document_id), UUID(project_id))
            )
            logger.info(f"[Celery Reindex] Document {document_id} reindexed successfully")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"[Celery Reindex] Error reindexing document {document_id}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


async def reindex_document_async(document_id: UUID, project_id: UUID):
    """–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ PostgreSQL –≤ Qdrant"""
    async with AsyncSessionLocal() as db:
        from app.models.document import Document, DocumentChunk
        from app.services.embedding_service import EmbeddingService
        from app.vector_db.vector_store import VectorStore
        from sqlalchemy import select
        from qdrant_client.models import PointStruct
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ PostgreSQL
        result = await db.execute(
            select(DocumentChunk)
            .where(DocumentChunk.document_id == document_id)
            .order_by(DocumentChunk.chunk_index)
        )
        chunks = result.scalars().all()
        
        if not chunks:
            # –ï—Å–ª–∏ –Ω–µ—Ç —á–∞–Ω–∫–æ–≤, –ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_result = await db.execute(select(Document).where(Document.id == document_id))
            document = doc_result.scalar_one_or_none()
            
            if document and document.content:
                from app.documents.chunker import DocumentChunker
                chunker = DocumentChunker(chunk_size=800, chunk_overlap=200)
                text_chunks = chunker.chunk_text(document.content)
                
                chunks = []
                for i, text in enumerate(text_chunks):
                    chunk = DocumentChunk(
                        document_id=document_id,
                        chunk_text=text,
                        chunk_index=i
                    )
                    db.add(chunk)
                    await db.flush()
                    chunks.append(chunk)
                
                await db.commit()
                logger.info(f"[Reindex] Created {len(chunks)} chunks from document content")
        
        if not chunks:
            logger.warning(f"[Reindex] No chunks found for document {document_id}")
            return
        
        embedding_service = EmbeddingService()
        vector_store = VectorStore()
        collection_name = f"project_{project_id}"
        
        batch_points = []
        
        for chunk in chunks:
            try:
                embedding = await embedding_service.create_embedding(chunk.chunk_text)
                
                batch_points.append(PointStruct(
                    id=str(chunk.id),
                    vector=embedding,
                    payload={
                        "document_id": str(document_id),
                        "chunk_id": str(chunk.id),
                        "chunk_index": chunk.chunk_index,
                        "chunk_text": chunk.chunk_text[:500]
                    }
                ))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á–∞–º–∏
                if len(batch_points) >= 20:
                    await vector_store.ensure_collection(collection_name, len(embedding))
                    vector_store.client.upsert(
                        collection_name=collection_name,
                        points=batch_points
                    )
                    logger.info(f"[Reindex] Upserted batch of {len(batch_points)} points")
                    batch_points = []
                    
            except Exception as e:
                logger.error(f"[Reindex] Error processing chunk {chunk.id}: {e}")
                continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ—á–∫–∏
        if batch_points:
            await vector_store.ensure_collection(collection_name, len(embedding))
            vector_store.client.upsert(
                collection_name=collection_name,
                points=batch_points
            )
            logger.info(f"[Reindex] Upserted final batch of {len(batch_points)} points")
        
        logger.info(f"[Reindex] Document {document_id} reindexed: {len(chunks)} chunks")

