"""
Celery –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–≥—Ä—É–∑–∫—É –≤ PostgreSQL –∏ Qdrant
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""
import os
import gc
import logging
from uuid import UUID
from celery import Task
from app.celery_app import celery_app
from app.core.database import AsyncSessionLocal

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
LARGE_DOCUMENT_THRESHOLD = 500_000  # 500KB —Ç–µ–∫—Å—Ç–∞
VERY_LARGE_DOCUMENT_THRESHOLD = 2_000_000  # 2MB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 200+ —Å—Ç—Ä–∞–Ω–∏—Ü)
MAX_BATCH_SIZE_LARGE = 5  # –ú–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
MAX_BATCH_SIZE_NORMAL = 10  # –û–±—ã—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
MAX_BATCH_SIZE_VERY_LARGE = 3  # –ï—â–µ –º–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
PDF_PAGES_PER_BATCH = 50  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF –ø–æ 50 —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑


class DatabaseTask(Task):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –ë–î"""
    _db = None

    @property
    def db(self):
        if self._db is None:
            # –î–ª—è Celery –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –ë–î
            # –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
            pass
        return self._db


@celery_app.task(bind=True, name='app.tasks.document_tasks.process_document_task')
def process_document_task(self, document_id: str, project_id: str, file_path: str, filename: str, file_type: str):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
    –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –≤–æ—Ä–∫–µ—Ä–µ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è out of memory
    """
    import asyncio
    import psutil
    
    process = psutil.Process(os.getpid())
    start_memory = process.memory_info().rss / 1024 / 1024
    logger.info(f"[Celery] Starting processing document {document_id} ({filename}), memory: {start_memory:.2f}MB")
    
    file_content = None
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if not os.path.exists(file_path):
            logger.error(f"[Celery] File not found: {file_path}")
            return {"status": "error", "message": f"File not found: {file_path}"}
        
        file_size = os.path.getsize(file_path) / 1024 / 1024
        logger.info(f"[Celery] Reading file {file_path}, size: {file_size:.2f}MB")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        read_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Celery] File read into memory, memory: {read_memory:.2f}MB (delta: {read_memory - start_memory:.2f}MB)")
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —á—Ç–µ–Ω–∏—è
        try:
            os.unlink(file_path)
            logger.info(f"[Celery] Temp file deleted: {file_path}")
        except Exception as e:
            logger.warning(f"[Celery] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {file_path}: {e}")
        
        # –í—ã–∑—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        # Celery –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.run –¥–ª—è async —Ñ—É–Ω–∫—Ü–∏–π
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                process_document_async(
                    UUID(document_id),
                    UUID(project_id),
                    file_content,
                    filename,
                    file_type
                )
            )
            logger.info(f"[Celery] Document {document_id} processed successfully")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"[Celery] Error processing document {document_id}: {e}", exc_info=True)
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if file_path and os.path.exists(file_path):
            try:
                os.unlink(file_path)
            except:
                pass
        return {"status": "error", "message": str(e)}
    finally:
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
        if file_content is not None:
            del file_content
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Celery] Processing complete for document {document_id}, final memory: {final_memory:.2f}MB")


async def process_document_async(document_id: UUID, project_id: UUID, file_content: bytes, filename: str, file_type: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–∞—Ä—Å–∏–Ω–≥, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant)"""
    import gc
    import psutil
    
    process = psutil.Process(os.getpid())
    try:
        async with AsyncSessionLocal() as db:
            # –ü–∞—Ä—Å–∏–Ω–≥ –∏ —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            from app.documents.parser import DocumentParser
            from app.documents.chunker import DocumentChunker
            from app.models.document import Document
            from sqlalchemy import select
            
            parser = DocumentParser()
            chunker = DocumentChunker()
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ü–ï–†–ï–î –ø–∞—Ä—Å–∏–Ω–≥–æ–º, aby m√≥c szybko zaktualizowaƒá content
            result = await db.execute(select(Document).where(Document.id == document_id))
            document = result.scalar_one_or_none()
            if not document:
                logger.error(f"[Celery] Document {document_id} not found")
                return
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"[Celery] üîÑ –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} ({filename})")
            logger.info(f"[Celery]   - –¢–∏–ø —Ñ–∞–π–ª–∞: {file_type}")
            logger.info(f"[Celery]   - –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(file_content) / 1024 / 1024:.2f} MB")
            
            try:
                text = await parser.parse(file_content, file_type)
                logger.info(f"[Celery] ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                if not text or len(text.strip()) < 50:
                    logger.error(f"[Celery] ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–∞—Ä—Å–∏–Ω–≥ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç!")
                    logger.error(f"[Celery] ‚ùå –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text) if text else 0} —Å–∏–º–≤–æ–ª–æ–≤")
                    logger.error(f"[Celery] ‚ùå –≠—Ç–æ –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å, —á—Ç–æ PDF —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –ø–∞—Ä—Å–µ—Ä –Ω–µ —Å–º–æ–≥ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç")
                    raise ValueError(f"–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç: {len(text) if text else 0} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                preview = text[:500] if len(text) > 500 else text
                logger.info(f"[Celery] üìÑ –ü—Ä–µ–≤—å—é –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {preview}...")
                
            except Exception as e:
                logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} ({filename}): {e}", exc_info=True)
                logger.error(f"[Celery] ‚ùå –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
                logger.error(f"[Celery] ‚ùå –°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
                document.content = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:200]}"
                await db.commit()
                return
            
            # –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º content –ù–ï–ú–ï–î–õ–ï–ù–ù–û –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á—Ç–æ–±—ã RAG –º–æ–≥ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞: 2MB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 2,000,000 —Å–∏–º–≤–æ–ª–æ–≤)
            logger.info(f"[Celery] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ RAG...")
            MAX_CONTENT_SIZE = 2_000_000
            if len(text) > MAX_CONTENT_SIZE:
                logger.warning(f"[Celery] ‚ö†Ô∏è Document {document_id} content —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {MAX_CONTENT_SIZE}")
                document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
            else:
                document.content = text
            
            # –ö–û–ú–ú–ò–¢–ò–ú –°–†–ê–ó–£, —á—Ç–æ–±—ã content by≈Ç dostƒôp–µ–Ω –¥–ª—è RAG
            logger.info(f"[Celery] üíæ –ö–æ–º–º–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î...")
            await db.commit()
            await db.refresh(document)
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
            saved_content = document.content
            saved_content_length = len(saved_content) if saved_content else 0
            
            logger.info(f"[Celery] ‚úÖ‚úÖ‚úÖ –î–û–ö–£–ú–ï–ù–¢ –ì–û–¢–û–í –î–õ–Ø RAG –ó–ê–ü–†–û–°–û–í ‚úÖ‚úÖ‚úÖ")
            logger.info(f"[Celery] üìÑ Document ID: {document_id}")
            logger.info(f"[Celery] üìÑ Filename: {filename}")
            logger.info(f"[Celery] üìÑ Text length (–∏–∑–≤–ª–µ—á–µ–Ω–æ): {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery] üìÑ Content saved (–≤ –ë–î): {saved_content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery] üìÑ Content status: {'READY' if saved_content_length > 100 else 'EMPTY/ERROR'}")
            logger.info(f"[Celery] üìÑ Content is '–û–±—Ä–∞–±–æ—Ç–∫–∞...': {saved_content == '–û–±—Ä–∞–±–æ—Ç–∫–∞...'}")
            logger.info(f"[Celery] üìÑ Document is now READY for RAG queries - –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã!")
            
            if saved_content_length < 100:
                logger.error(f"[Celery] ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è –≤ –ë–î!")
                logger.error(f"[Celery] ‚ùå –û–∂–∏–¥–∞–ª–æ—Å—å: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                logger.error(f"[Celery] ‚ùå –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_content_length} —Å–∏–º–≤–æ–ª–æ–≤")
                logger.error(f"[Celery] ‚ùå –ó–Ω–∞—á–µ–Ω–∏–µ content: '{saved_content[:200] if saved_content else 'EMPTY'}...'")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if document.content and saved_content_length > 100:
                preview = document.content[:500] if len(document.content) > 500 else document.content
                logger.info(f"[Celery] üìÑ Content preview (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤): {preview}...")
            else:
                logger.warning(f"[Celery] ‚ö†Ô∏è Content preview –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π)")
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ (1000/200)
            logger.info(f"[Celery] üî™ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–±–∏–≤–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏: {filename}, —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            from app.documents.advanced_chunker import AdvancedChunker
            advanced_chunker = AdvancedChunker(
                default_chunk_size=1000,  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
                default_overlap=200,  # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
                min_chunk_size=100,
                max_chunk_size=2000
            )
            
            chunking_start_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Celery] üî™ –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ chunking: {chunking_start_memory:.2f}MB")
            
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunking
            chunks = await advanced_chunker.chunk_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename
            )
            
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π chunker –µ—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            if not chunks or len(chunks) == 0:
                logger.warning(f"[Celery] ‚ö†Ô∏è Advanced chunking failed, using simple chunker with correct params (1000/200)")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunking
                chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)
                chunks = chunker.chunk_text(text)
            if not chunks:
                logger.warning(f"[Celery] ‚ùå –î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ chunking")
                return
            
            chunking_end_memory = process.memory_info().rss / 1024 / 1024
            total_chunks = len(chunks)
            avg_chunk_size = sum(len(c) for c in chunks) / total_chunks if chunks else 0
            logger.info(f"[Celery] ‚úÖ Document split into {total_chunks} chunks:")
            logger.info(f"[Celery]   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery]   - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(len(c) for c in chunks) if chunks else 0} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery]   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(len(c) for c in chunks) if chunks else 0} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[Celery]   - –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ chunking: {chunking_end_memory:.2f}MB (delta: {chunking_end_memory - chunking_start_memory:.2f}MB)")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
            from app.services.embedding_service import EmbeddingService
            from app.vector_db.vector_store import VectorStore
            from app.models.document import DocumentChunk
            
            embedding_service = EmbeddingService()
            vector_store = VectorStore()
            
            # Batch processing –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
            EMBEDDING_BATCH_SIZE = 100  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
            QDRANT_BATCH_SIZE = 100  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è Qdrant (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
            batch_points = []
            batch_chunks = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            logger.info(f"[Celery] üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(chunks)} —á–∞–Ω–∫–æ–≤: —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant")
            embedding_start_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Celery] üìä –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embedding_start_memory:.2f}MB")
            
            successful_chunks = 0
            failed_chunks = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –±–∞—Ç—á–∞–º–∏
            for batch_start in range(0, len(chunks), EMBEDDING_BATCH_SIZE):
                batch_chunk_texts = chunks[batch_start:batch_start + EMBEDDING_BATCH_SIZE]
                batch_indices = list(range(batch_start, min(batch_start + EMBEDDING_BATCH_SIZE, len(chunks))))
                
                try:
                    logger.info(f"[Celery] üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_start // EMBEDDING_BATCH_SIZE + 1}: —á–∞–Ω–∫–∏ {batch_start + 1}-{batch_start + len(batch_chunk_texts)} –∏–∑ {len(chunks)}")
                    
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞ (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
                    try:
                        embeddings = await embedding_service.create_embeddings_batch(batch_chunk_texts)
                        logger.info(f"[Celery] ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –±–∞—Ç—á–∞: {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                    except Exception as e:
                        logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞: {e}, –ø—Ä–æ–±—É–µ–º –ø–æ –æ–¥–Ω–æ–º—É")
                        # Fallback: —Å–æ–∑–¥–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É
                        embeddings = []
                        for chunk_text in batch_chunk_texts:
                            try:
                                emb = await embedding_service.create_embedding(chunk_text)
                                embeddings.append(emb)
                            except Exception as single_error:
                                logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞: {single_error}")
                                embeddings.append(None)
                                failed_chunks += 1
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–∞
                    for chunk_index, (chunk_text, embedding) in enumerate(zip(batch_chunk_texts, embeddings)):
                        actual_index = batch_indices[chunk_index]
                        
                        if embedding is None:
                            logger.warning(f"[Celery] ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —á–∞–Ω–∫–∞ {actual_index + 1}: —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ —Å–æ–∑–¥–∞–Ω")
                            failed_chunks += 1
                            continue
                        
                        try:
                    
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ –≤ –ë–î (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞)
                            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 10KB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 10,000 —Å–∏–º–≤–æ–ª–æ–≤)
                            MAX_CHUNK_SIZE = 10_000
                            chunk_text_to_save = chunk_text[:MAX_CHUNK_SIZE] if len(chunk_text) > MAX_CHUNK_SIZE else chunk_text
                            if len(chunk_text) > MAX_CHUNK_SIZE:
                                logger.warning(f"[Celery] ‚ö†Ô∏è Chunk {actual_index + 1} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({len(chunk_text)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {MAX_CHUNK_SIZE}")
                            
                            chunk = DocumentChunk(
                                document_id=document_id,
                                chunk_text=chunk_text_to_save,
                                chunk_index=actual_index
                            )
                            db.add(chunk)
                            await db.flush()  # –ü–æ–ª—É—á–∞–µ–º ID —á–∞–Ω–∫–∞
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á –¥–ª—è Qdrant (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
                            from qdrant_client.models import PointStruct
                            import hashlib
                            
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
                            chunk_hash = hashlib.md5(chunk_text.encode()).hexdigest()
                            point_id = abs(hash(f"{document_id}_{actual_index}_{chunk_hash}")) % (10 ** 10)
                            
                            batch_points.append(PointStruct(
                                id=point_id,
                                vector=embedding,
                                payload={
                                    "document_id": str(document_id),
                                    "chunk_id": str(chunk.id),
                                    "chunk_index": actual_index,
                                    "filename": filename,
                                    "chunk_text": chunk_text[:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è Qdrant
                                    "text": chunk_text[:500]  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                                }
                            ))
                            batch_chunks.append((chunk, point_id))
                            successful_chunks += 1
                            
                        except Exception as chunk_error:
                            logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {actual_index + 1}: {chunk_error}")
                            failed_chunks += 1
                            continue
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á –≤ Qdrant –∫–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á
                    if len(batch_points) >= QDRANT_BATCH_SIZE or batch_start + EMBEDDING_BATCH_SIZE >= len(chunks):
                            try:
                                # Batch upsert –≤ Qdrant (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
                                collection_name = f"project_{project_id}"
                                logger.info(f"[Celery] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞—Ç—á–∞ –∏–∑ {len(batch_points)} —á–∞–Ω–∫–æ–≤ –≤ Qdrant (–∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection_name})")
                                await vector_store.ensure_collection(collection_name, len(embedding))
                                vector_store.client.upsert(
                                    collection_name=collection_name,
                                    points=batch_points
                                )
                                
                                # –û–±–Ω–æ–≤–ª—è–µ–º qdrant_point_id –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –≤ –±–∞—Ç—á–µ
                                for batch_chunk, batch_point_id in batch_chunks:
                                    batch_chunk.qdrant_point_id = batch_point_id
                                await db.flush()
                                
                                progress_pct = ((batch_start + len(batch_chunk_texts)) / len(chunks)) * 100
                                logger.info(f"[Celery] ‚úÖ –ë–∞—Ç—á –∏–∑ {len(batch_points)} —á–∞–Ω–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ Qdrant (–ø—Ä–æ–≥—Ä–µ—Å—Å: {batch_start + len(batch_chunk_texts)}/{len(chunks)} = {progress_pct:.1f}%)")
                            except Exception as e:
                                logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ batch upsert –≤ Qdrant: {e}", exc_info=True)
                                # –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –∫–∞–∫ fallback
                                for batch_chunk, batch_point_id in batch_chunks:
                                    try:
                                        point_data = next((p for p in batch_points if str(p.id) == str(batch_point_id)), None)
                                        if point_data:
                                            await vector_store.store_vector(
                                                collection_name=f"project_{project_id}",
                                                vector=point_data.vector,
                                                payload=point_data.payload
                                            )
                                            batch_chunk.qdrant_point_id = batch_point_id
                                            successful_chunks += 1
                                            failed_chunks -= 1
                                    except Exception as fallback_error:
                                        logger.error(f"[Celery] ‚ùå Fallback —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–∞ {batch_chunk.chunk_index} —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {fallback_error}")
                            
                            # –û—á–∏—â–∞–µ–º –±–∞—Ç—á
                            batch_points = []
                            batch_chunks = []
                            
                except Exception as batch_error:
                    logger.error(f"[Celery] ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_start // EMBEDDING_BATCH_SIZE + 1}: {batch_error}", exc_info=True)
                    failed_chunks += len(batch_chunk_texts)
                    continue
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                chunk_memory_after = process.memory_info().rss / 1024 / 1024
                progress_pct = ((batch_start + len(batch_chunk_texts)) / len(chunks)) * 100
                logger.info(f"[Celery] üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {batch_start + len(batch_chunk_texts)}/{len(chunks)} —á–∞–Ω–∫–æ–≤ ({progress_pct:.1f}%), –ø–∞–º—è—Ç—å: {chunk_memory_after:.2f}MB, —É—Å–ø–µ—à–Ω–æ: {successful_chunks}, –æ—à–∏–±–æ–∫: {failed_chunks}")
                
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                gc.collect()
            
            embedding_end_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Celery] ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
            logger.info(f"[Celery]   - –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            logger.info(f"[Celery]   - –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_chunks}")
            logger.info(f"[Celery]   - –û—à–∏–±–æ–∫: {failed_chunks}")
            logger.info(f"[Celery]   - –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {embedding_end_memory:.2f}MB (delta: {embedding_end_memory - embedding_start_memory:.2f}MB)")
            
            # –ö–æ–º–º–∏—Ç–∏–º –≤—Å–µ —á–∞–Ω–∫–∏
            await db.commit()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"[Celery] üîç –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}...")
            result = await db.execute(select(Document).where(Document.id == document_id))
            document = result.scalar_one_or_none()
            if document:
                content_length = len(document.content) if document.content else 0
                content_value = document.content if document.content else "EMPTY"
                is_processing = content_value in ["–û–±—Ä–∞–±–æ—Ç–∫–∞...", "–û–±—Ä–∞–±–æ—Ç–∞–Ω", ""] or content_length < 100
                
                logger.info(f"[Celery] ‚úÖ Document {document_id} processed successfully:")
                logger.info(f"  - Filename: {filename}")
                logger.info(f"  - Chunks created: {len(chunks)}")
                logger.info(f"  - Content length: {content_length} chars")
                logger.info(f"  - Content status: {'READY' if not is_processing else 'NOT_READY'}")
                logger.info(f"  - Content is '–û–±—Ä–∞–±–æ—Ç–∫–∞...': {content_value == '–û–±—Ä–∞–±–æ—Ç–∫–∞...'}")
                logger.info(f"  - Content is empty: {not content_value or content_value == ''}")
                logger.info(f"  - Content preview (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {content_value[:200] if content_value and len(content_value) > 200 else content_value}...")
                
                if is_processing:
                    logger.error(f"[Celery] ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: –î–æ–∫—É–º–µ–Ω—Ç –≤—Å–µ –µ—â–µ –≤ —Å—Ç–∞—Ç—É—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏!")
                    logger.error(f"[Celery] ‚ùå –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –±—ã–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∏–ª–∏ –±—ã–ª –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω")
                    logger.error(f"[Celery] ‚ùå –ó–Ω–∞—á–µ–Ω–∏–µ content: '{content_value}'")
                    logger.error(f"[Celery] ‚ùå –î–ª–∏–Ω–∞: {content_length} —Å–∏–º–≤–æ–ª–æ–≤")
                else:
                    logger.info(f"[Celery] ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –¥–ª—è RAG - –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
                    
                if document.content and content_length > 0 and not is_processing:
                    preview = document.content[:300] if content_length > 300 else document.content
                    logger.info(f"  - Content preview: {preview}...")
            else:
                logger.error(f"[Celery] ‚ùå Document {document_id} not found after processing!")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM (–≤ —Ñ–æ–Ω–µ)
            try:
                from app.services.document_summary_service import DocumentSummaryService
                summary_service = DocumentSummaryService(db)
                summary = await summary_service.generate_summary(document_id)
                if summary:
                    logger.info(f"[Celery] Summary generated for document {document_id}")
            except Exception as summary_error:
                logger.warning(f"[Celery] Error generating summary for document {document_id}: {summary_error}")
            
    except Exception as e:
        logger.error(f"[Celery] Error processing document {document_id}: {e}", exc_info=True)
    finally:
        gc.collect()


@celery_app.task(bind=True, name='app.tasks.document_tasks.generate_document_summary_task')
def generate_document_summary_task(self, document_id: str):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ summary –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ LLM
    """
    import asyncio
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async def generate_summary_async():
                async with AsyncSessionLocal() as db:
                    from app.services.document_summary_service import DocumentSummaryService
                    summary_service = DocumentSummaryService(db)
                    summary = await summary_service.generate_summary(UUID(document_id))
                    return summary
            
            result = loop.run_until_complete(generate_summary_async())
            logger.info(f"[Celery] Summary generated for document {document_id}")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"[Celery] Error generating summary for document {document_id}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


@celery_app.task(bind=True, name='app.tasks.document_tasks.process_large_document_with_langgraph')
def process_large_document_with_langgraph(
    self, 
    document_id: str, 
    project_id: str, 
    file_path: str, 
    filename: str, 
    file_type: str
):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangGraph
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è PDF >100 —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    import asyncio
    import psutil
    
    process = psutil.Process(os.getpid())
    start_memory = process.memory_info().rss / 1024 / 1024
    logger.info(f"[Celery LangGraph] Starting processing document {document_id} ({filename}), memory: {start_memory:.2f}MB")
    
    file_content = None
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        if not os.path.exists(file_path):
            logger.error(f"[Celery LangGraph] File not found: {file_path}")
            return {"status": "error", "message": f"File not found: {file_path}"}
        
        file_size = os.path.getsize(file_path) / 1024 / 1024
        logger.info(f"[Celery LangGraph] Reading file {file_path}, size: {file_size:.2f}MB")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        try:
            os.unlink(file_path)
            logger.info(f"[Celery LangGraph] Temp file deleted: {file_path}")
        except Exception as e:
            logger.warning(f"[Celery LangGraph] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {file_path}: {e}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                process_large_document_async_langgraph(
                    UUID(document_id),
                    UUID(project_id),
                    file_content,
                    filename,
                    file_type
                )
            )
            logger.info(f"[Celery LangGraph] Document {document_id} processed successfully")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
            
    except Exception as e:
        logger.error(f"[Celery LangGraph] Error processing document {document_id}: {e}", exc_info=True)
        if file_path and os.path.exists(file_path):
            try:
                os.unlink(file_path)
            except:
                pass
        return {"status": "error", "message": str(e)}
    finally:
        if file_content is not None:
            del file_content
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Celery LangGraph] Processing complete, final memory: {final_memory:.2f}MB")


async def process_large_document_async_langgraph(
    document_id: UUID, 
    project_id: UUID, 
    file_content: bytes, 
    filename: str, 
    file_type: str
):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å LangGraph
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö PDF
    """
    import gc
    import psutil
    
    process = psutil.Process(os.getpid())
    
    async with AsyncSessionLocal() as db:
        from app.documents.parser import DocumentParser
        from app.documents.advanced_chunker import AdvancedChunker
        from app.models.document import Document, DocumentChunk
        from sqlalchemy import select
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        result = await db.execute(select(Document).where(Document.id == document_id))
        document = result.scalar_one_or_none()
        if not document:
            logger.error(f"[Celery LangGraph] Document {document_id} not found")
            return
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        parser = DocumentParser()
        try:
            text = await parser.parse(file_content, file_type)
        except Exception as e:
            logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            document.content = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:200]}"
            await db.commit()
            return
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
        is_large_document = len(text) > LARGE_DOCUMENT_THRESHOLD
        is_very_large_document = len(text) > VERY_LARGE_DOCUMENT_THRESHOLD
        logger.info(f"[Celery LangGraph] Document size: {len(text)} chars, is_large: {is_large_document}, is_very_large: {is_very_large_document}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        MAX_CONTENT_SIZE = 2_000_000
        if len(text) > MAX_CONTENT_SIZE:
            document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
        else:
            document.content = text
        
        await db.commit()
        await db.refresh(document)
        logger.info(f"[Celery LangGraph] ‚úÖ Document content saved: {len(document.content)} chars")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º advanced chunker —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (1000/200 –¥–ª—è –æ–±—ã—á–Ω—ã—Ö, –±–æ–ª—å—à–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö)
        advanced_chunker = AdvancedChunker(
            default_chunk_size=1500 if is_large_document else 1000,  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
            default_overlap=300 if is_large_document else 200,  # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
            min_chunk_size=100,
            max_chunk_size=3000 if is_large_document else 2000
        )
        
        if is_large_document:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
            chunk_result = await advanced_chunker.chunk_large_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename,
                use_hierarchical=True
            )
            chunks = [c['text'] for c in chunk_result.get('chunks', [])]
            sections = chunk_result.get('sections', [])
            logger.info(f"[Celery LangGraph] Hierarchical chunking: {len(chunks)} chunks, {len(sections)} sections")
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π chunking
            chunks = await advanced_chunker.chunk_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename
            )
        
        if not chunks:
            logger.warning(f"[Celery LangGraph] No chunks generated for document {document_id}")
            return
        
        logger.info(f"[Celery LangGraph] Document split into {len(chunks)} chunks")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant
        from app.services.embedding_service import EmbeddingService
        from app.vector_db.vector_store import VectorStore
        from qdrant_client.models import PointStruct
        
        embedding_service = EmbeddingService()
        vector_store = VectorStore()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if is_very_large_document:
            batch_size = MAX_BATCH_SIZE_VERY_LARGE
            logger.info(f"[Celery LangGraph] –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á ({batch_size}) –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        elif is_large_document:
            batch_size = MAX_BATCH_SIZE_LARGE
        else:
            batch_size = MAX_BATCH_SIZE_NORMAL
        
        batch_points = []
        batch_chunks = []
        
        for chunk_index, chunk_text in enumerate(chunks):
            try:
                chunk_memory_before = process.memory_info().rss / 1024 / 1024
                
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                try:
                    embedding = await embedding_service.create_embedding(chunk_text)
                except Exception as e:
                    logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index}: {e}")
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ –≤ –ë–î
                MAX_CHUNK_SIZE = 10_000
                chunk_text_to_save = chunk_text[:MAX_CHUNK_SIZE] if len(chunk_text) > MAX_CHUNK_SIZE else chunk_text
                
                chunk = DocumentChunk(
                    document_id=document_id,
                    chunk_text=chunk_text_to_save,
                    chunk_index=chunk_index
                )
                db.add(chunk)
                await db.flush()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á –¥–ª—è Qdrant
                point_id = chunk.id
                batch_points.append(PointStruct(
                    id=str(point_id),
                    vector=embedding,
                    payload={
                        "document_id": str(document_id),
                        "chunk_id": str(chunk.id),
                        "chunk_index": chunk_index,
                        "filename": filename,
                        "chunk_text": chunk_text[:500]
                    }
                ))
                batch_chunks.append((chunk, point_id))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á
                if len(batch_points) >= batch_size or chunk_index == len(chunks) - 1:
                    try:
                        collection_name = f"project_{project_id}"
                        await vector_store.ensure_collection(collection_name, len(embedding))
                        vector_store.client.upsert(
                            collection_name=collection_name,
                            points=batch_points
                        )
                        
                        for batch_chunk, batch_point_id in batch_chunks:
                            batch_chunk.qdrant_point_id = batch_point_id
                        await db.flush()
                        
                        logger.info(f"[Celery LangGraph] ‚úÖ Batch upserted {len(batch_points)} chunks (up to {chunk_index})")
                    except Exception as e:
                        logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ batch upsert –≤ Qdrant: {e}")
                    
                    batch_points = []
                    batch_chunks = []
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (—á–∞—â–µ –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
                log_interval = 10 if is_very_large_document else 20
                if chunk_index % log_interval == 0:
                    chunk_memory_after = process.memory_info().rss / 1024 / 1024
                    progress_pct = (chunk_index + 1) / len(chunks) * 100
                    logger.info(f"[Celery LangGraph] Processed {chunk_index + 1}/{len(chunks)} ({progress_pct:.1f}%), memory: {chunk_memory_after:.2f}MB")
                    gc.collect()
                
            except Exception as e:
                logger.error(f"[Celery LangGraph] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {chunk_index}: {e}", exc_info=True)
                continue
        
        # –ö–æ–º–º–∏—Ç–∏–º –≤—Å–µ —á–∞–Ω–∫–∏
        await db.commit()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangGraph
        try:
            from app.services.document_summary_service import DocumentSummaryService
            summary_service = DocumentSummaryService(db)
            
            if is_large_document:
                # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º Map-Reduce
                summary = await summary_service.generate_map_reduce_summary(document_id)
            else:
                # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º LangGraph
                summary = await summary_service.generate_summary_with_langgraph(document_id)
            
            if summary:
                logger.info(f"[Celery LangGraph] Summary generated for document {document_id}")
        except Exception as summary_error:
            logger.warning(f"[Celery LangGraph] Error generating summary: {summary_error}")
        
        logger.info(f"[Celery LangGraph] ‚úÖ Document {document_id} processing complete: {len(chunks)} chunks")


@celery_app.task(bind=True, name='app.tasks.document_tasks.reindex_document_to_qdrant')
def reindex_document_to_qdrant(self, document_id: str, project_id: str):
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant
    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ PostgreSQL –≤ Qdrant
    """
    import asyncio
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                reindex_document_async(UUID(document_id), UUID(project_id))
            )
            logger.info(f"[Celery Reindex] Document {document_id} reindexed successfully")
            return {"status": "success", "document_id": document_id}
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"[Celery Reindex] Error reindexing document {document_id}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


async def reindex_document_async(document_id: UUID, project_id: UUID):
    """–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ PostgreSQL –≤ Qdrant"""
    async with AsyncSessionLocal() as db:
        from app.models.document import Document, DocumentChunk
        from app.services.embedding_service import EmbeddingService
        from app.vector_db.vector_store import VectorStore
        from sqlalchemy import select
        from qdrant_client.models import PointStruct
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ PostgreSQL
        result = await db.execute(
            select(DocumentChunk)
            .where(DocumentChunk.document_id == document_id)
            .order_by(DocumentChunk.chunk_index)
        )
        chunks = result.scalars().all()
        
        if not chunks:
            # –ï—Å–ª–∏ –Ω–µ—Ç —á–∞–Ω–∫–æ–≤, –ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_result = await db.execute(select(Document).where(Document.id == document_id))
            document = doc_result.scalar_one_or_none()
            
            if document and document.content:
                from app.documents.chunker import DocumentChunker
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunking (1000/200)
                chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)
                text_chunks = chunker.chunk_text(document.content)
                
                chunks = []
                for i, text in enumerate(text_chunks):
                    chunk = DocumentChunk(
                        document_id=document_id,
                        chunk_text=text,
                        chunk_index=i
                    )
                    db.add(chunk)
                    await db.flush()
                    chunks.append(chunk)
                
                await db.commit()
                logger.info(f"[Reindex] Created {len(chunks)} chunks from document content")
        
        if not chunks:
            logger.warning(f"[Reindex] No chunks found for document {document_id}")
            return
        
        embedding_service = EmbeddingService()
        vector_store = VectorStore()
        collection_name = f"project_{project_id}"
        
        batch_points = []
        
        for chunk in chunks:
            try:
                embedding = await embedding_service.create_embedding(chunk.chunk_text)
                
                batch_points.append(PointStruct(
                    id=str(chunk.id),
                    vector=embedding,
                    payload={
                        "document_id": str(document_id),
                        "chunk_id": str(chunk.id),
                        "chunk_index": chunk.chunk_index,
                        "chunk_text": chunk.chunk_text[:500]
                    }
                ))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á–∞–º–∏
                if len(batch_points) >= 20:
                    await vector_store.ensure_collection(collection_name, len(embedding))
                    vector_store.client.upsert(
                        collection_name=collection_name,
                        points=batch_points
                    )
                    logger.info(f"[Reindex] Upserted batch of {len(batch_points)} points")
                    batch_points = []
                    
            except Exception as e:
                logger.error(f"[Reindex] Error processing chunk {chunk.id}: {e}")
                continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ—á–∫–∏
        if batch_points:
            await vector_store.ensure_collection(collection_name, len(embedding))
            vector_store.client.upsert(
                collection_name=collection_name,
                points=batch_points
            )
            logger.info(f"[Reindex] Upserted final batch of {len(batch_points)} points")
        
        logger.info(f"[Reindex] Document {document_id} reindexed: {len(chunks)} chunks")

