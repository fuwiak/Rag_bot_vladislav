"""
–†–æ—É—Ç–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
"""
from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List
from uuid import UUID
import logging

from app.core.database import get_db, AsyncSessionLocal
from app.schemas.document import DocumentResponse
from app.services.document_service import DocumentService
from app.api.dependencies import get_current_admin

logger = logging.getLogger(__name__)

router = APIRouter()


async def process_document_async_from_file(document_id: UUID, project_id: UUID, file_path: str, filename: str, file_type: str):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ (–ø–∞—Ä—Å–∏–Ω–≥, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤–º–µ—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    """
    import os
    import gc
    import psutil
    
    process = psutil.Process(os.getpid())
    start_memory = process.memory_info().rss / 1024 / 1024
    logger.info(f"[Process] Starting processing document {document_id} ({filename}), memory: {start_memory:.2f}MB")
    
    file_content = None
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(file_path) / 1024 / 1024
        logger.info(f"[Process] Reading file {file_path}, size: {file_size:.2f}MB")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        def read_file_sync(path):
            with open(path, 'rb') as f:
                return f.read()
        
        import asyncio
        loop = asyncio.get_event_loop()
        file_content = await loop.run_in_executor(None, read_file_sync, file_path)
        
        read_memory = process.memory_info().rss / 1024 / 1024
        logger.info(f"[Process] File read into memory, memory: {read_memory:.2f}MB (delta: {read_memory - start_memory:.2f}MB)")
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —á—Ç–µ–Ω–∏—è (–æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –¥–∏—Å–∫)
        try:
            os.unlink(file_path)
            logger.info(f"[Process] Temp file deleted: {file_path}")
        except Exception as e:
            logger.warning(f"[Process] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {file_path}: {e}")
        
        # –í—ã–∑—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É, —á—Ç–æ–±—ã –¥–∞—Ç—å –≤—Ä–µ–º—è –æ—Å–Ω–æ–≤–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è
        await asyncio.sleep(0.1)
        await process_document_async(document_id, project_id, file_content, filename, file_type)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}", exc_info=True)
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if os.path.exists(file_path):
            try:
                os.unlink(file_path)
            except:
                pass
    finally:
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
        if file_content is not None:
            del file_content
        gc.collect()


async def process_document_async(document_id: UUID, project_id: UUID, file_content: bytes, filename: str, file_type: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–∞—Ä—Å–∏–Ω–≥, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant)"""
    import asyncio
    import gc  # –î–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
    import os
    import psutil
    
    process = psutil.Process(os.getpid())
    try:
        async with AsyncSessionLocal() as db:
            # –ü–∞—Ä—Å–∏–Ω–≥ –∏ —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
            from app.documents.parser import DocumentParser
            from app.documents.chunker import DocumentChunker
            from app.models.document import Document
            from sqlalchemy import select
            
            parser = DocumentParser()
            chunker = DocumentChunker()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ thread pool –¥–ª—è PDF/DOCX)
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á—Ç–æ–±—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –ø–∞–¥–∞–ª–æ
            try:
                text = await parser.parse(file_content, file_type)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} ({filename}): {e}")
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—à–∏–±–∫—É
                result = await db.execute(select(Document).where(Document.id == document_id))
                document = result.scalar_one_or_none()
                if document:
                    document.content = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:200]}"
                    await db.commit()
                return  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –Ω–æ –Ω–µ –ø–∞–¥–∞–µ–º
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
            if not text or not text.strip():
                logger.error(f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} ({filename}) –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç - –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å. PDF –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º/–Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.")
                result = await db.execute(select(Document).where(Document.id == document_id))
                document = result.scalar_one_or_none()
                if document:
                    document.content = "–û—à–∏–±–∫–∞: –¥–æ–∫—É–º–µ–Ω—Ç –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç. PDF –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
                    await db.commit()
                return
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –µ–≥–æ content
            result = await db.execute(select(Document).where(Document.id == document_id))
            document = result.scalar_one_or_none()
            
            if not document:
                logger.error(f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å —Ä–∞–∑—É–º–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞: 2MB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 2,000,000 —Å–∏–º–≤–æ–ª–æ–≤)
            MAX_CONTENT_SIZE = 2_000_000
            if len(text) > MAX_CONTENT_SIZE:
                logger.warning(f"[Process] Document {document_id} content too large ({len(text)} chars), truncating to {MAX_CONTENT_SIZE}")
                document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
            else:
                document.content = text
            
            await db.commit()
            await db.refresh(document)
            
            logger.info(f"[Process] Document parsed and saved - ID: {document_id}, Filename: {filename}, "
                       f"Text length: {len(text)} chars, Content saved: {len(document.content)} chars")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if document.content:
                preview = document.content[:500] if len(document.content) > 500 else document.content
                logger.info(f"[Process] Document content preview (first 500 chars): {preview}...")
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º file_content —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
            if 'file_content' in locals():
                del file_content
            gc.collect()
            
            parse_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Process] After parse, memory: {parse_memory:.2f}MB, text_length: {len(text)} chars")
            
            # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunker —Å fallback-–∞–º–∏
            from app.documents.advanced_chunker import AdvancedChunker
            advanced_chunker = AdvancedChunker(
                default_chunk_size=800,
                default_overlap=200,
                min_chunk_size=100,
                max_chunk_size=2000
            )
            
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π chunking
            chunks = await advanced_chunker.chunk_document(
                text=text,
                file_type=file_type,
                file_content=file_content if file_type == "pdf" else None,
                filename=filename
            )
            
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π chunker –µ—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            if not chunks or len(chunks) == 0:
                logger.warning(f"[Process] Advanced chunking failed, using simple chunker")
                chunks = chunker.chunk_text(text)
            
            # –ù–ï —É–¥–∞–ª—è–µ–º text —Å—Ä–∞–∑—É - –æ–Ω –Ω—É–∂–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            # –û—Å–≤–æ–±–æ–¥–∏–º –µ–≥–æ –ø–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            
            if not chunks:
                logger.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
                return
            
            logger.info(f"[Process] Document split into {len(chunks)} chunks")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª–∏–Ω–Ω—ã–º (–±–æ–ª–µ–µ 100,000 —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ –±–æ–ª–µ–µ 100 —á–∞–Ω–∫–æ–≤)
            # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LongDocumentService
            is_long_document = len(text) > 100_000 or len(chunks) > 100
            
            if is_long_document:
                logger.info(f"[Process] Long document detected ({len(text)} chars, {len(chunks)} chunks). Using LongDocumentService for batch processing...")
                from app.services.long_document_service import LongDocumentService
                long_doc_service = LongDocumentService(db)
                total_processed = await long_doc_service.process_long_document(
                    document_id=document.id,
                    project_id=project_id,
                    text=text,
                    batch_size=50
                )
                logger.info(f"[Process] Long document processing complete: {total_processed} chunks processed")
                return
            
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ –æ–¥–Ω–æ–º—É —á–∞–Ω–∫—É
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            from app.services.embedding_service import EmbeddingService
            from app.vector_db.vector_store import VectorStore
            from app.models.document import DocumentChunk
            
            embedding_service = EmbeddingService()
            vector_store = VectorStore()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            # –≠—Ç–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç out of memory
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            for chunk_index, chunk_text in enumerate(chunks):
                try:
                    chunk_memory_before = process.memory_info().rss / 1024 / 1024
                    
                    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
                    try:
                        embedding = await embedding_service.create_embedding(chunk_text)
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index}: {e}")
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —á–∞–Ω–∫, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
                        continue
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –≤ –ë–î (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞)
                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 10KB —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 10,000 —Å–∏–º–≤–æ–ª–æ–≤)
                    MAX_CHUNK_SIZE = 10_000
                    chunk_text_to_save = chunk_text[:MAX_CHUNK_SIZE] if len(chunk_text) > MAX_CHUNK_SIZE else chunk_text
                    if len(chunk_text) > MAX_CHUNK_SIZE:
                        logger.warning(f"[Process] Chunk {chunk_index} too large ({len(chunk_text)} chars), truncating to {MAX_CHUNK_SIZE}")
                    
                    chunk = DocumentChunk(
                        document_id=document.id,
                        chunk_text=chunk_text_to_save,
                        chunk_index=chunk_index
                    )
                    db.add(chunk)
                    await db.flush()
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –≤ Qdrant
                    try:
                        point_id = await vector_store.store_vector(
                            collection_name=f"project_{project_id}",
                            vector=embedding,
                            payload={
                                "document_id": str(document.id),
                                "chunk_id": str(chunk.id),
                                "chunk_index": chunk_index,
                                "chunk_text": chunk_text[:500]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –≤ payload
                            }
                        )
                        chunk.qdrant_point_id = point_id
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –≤ Qdrant –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index}: {e}")
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –¥–∞–∂–µ –µ—Å–ª–∏ Qdrant –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                    
                    await db.commit()
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
                    del embedding
                    del chunk_text
                    gc.collect()
                    
                    chunk_memory_after = process.memory_info().rss / 1024 / 1024
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤
                    if (chunk_index + 1) % 10 == 0:
                        logger.info(f"[Process] Processed {chunk_index + 1}/{len(chunks)} chunks, memory: {chunk_memory_after:.2f}MB")
                    
                    # –ü–∞—É–∑–∞ –∫–∞–∂–¥—ã–µ 5 —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
                    if (chunk_index + 1) % 5 == 0:
                        await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"[Process] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞ {chunk_index}: {e}")
                    # –ü—Ä–æ–±—É–µ–º –æ—Ç–∫–∞—Ç–∏—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å
                    try:
                        await db.rollback()
                    except:
                        pass
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤
                    continue
            
            final_memory = process.memory_info().rss / 1024 / 1024
            logger.info(f"[Process] –î–æ–∫—É–º–µ–Ω—Ç {document_id} ({filename}) —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(chunks)} —á–∞–Ω–∫–æ–≤, final memory: {final_memory:.2f}MB")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —É—Å–ø–µ—à–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: content —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω –≤—ã—à–µ, –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
            result = await db.execute(select(Document).where(Document.id == document_id))
            document = result.scalar_one_or_none()
            if document:
                if document.content == "–û–±—Ä–∞–±–æ—Ç–∫–∞..." or len(document.content) < 100:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—â–µ placeholder –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                    # –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ø–µ—Ä–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ
                    MAX_CONTENT_SIZE = 2_000_000
                    if len(text) > MAX_CONTENT_SIZE:
                        document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
                    else:
                        document.content = text
                    logger.info(f"[Process] Document content updated in final step - {len(document.content)} chars")
                
                # Summary –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Celery –∑–∞–¥–∞—á–µ
                
                await db.commit()
                logger.info(f"[Process] Document {document_id} final status - Content length: {len(document.content) if document.content else 0} chars")
            
            # –¢–µ–ø–µ—Ä—å –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º text –ø–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            if 'text' in locals():
                del text
            gc.collect()
                
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}", exc_info=True)
        # –ü—Ä–æ–±—É–µ–º –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—à–∏–±–∫—É
        try:
            from sqlalchemy import select
            from app.models.document import Document
            async with AsyncSessionLocal() as db:
                result = await db.execute(select(Document).where(Document.id == document_id))
                document = result.scalar_one_or_none()
                if document:
                    document.content = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:200]}"
                    await db.commit()
        except Exception as e2:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {e2}")
    finally:
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        # file_content —É–∂–µ —É–¥–∞–ª–µ–Ω –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
        if 'text' in locals():
            del text
        if 'chunks' in locals():
            del chunks
        gc.collect()


@router.post("/{project_id}/upload", response_model=List[DocumentResponse], status_code=status.HTTP_201_CREATED)
async def upload_documents(
    project_id: UUID,
    files: List[UploadFile] = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: AsyncSession = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø—Ä–æ–µ–∫—Ç"""
    import psutil
    import os
    from app.core.config import settings
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 50MB (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    MAX_FILE_SIZE = getattr(settings, 'MAX_DOCUMENT_SIZE', 50 * 1024 * 1024)  # 50MB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    logger.info(f"[Upload] Starting upload for project {project_id}, files: {len(files)}, initial memory: {initial_memory:.2f}MB")
    
    service = DocumentService(db)
    
    documents = []
    for file_index, file in enumerate(files):
        logger.info(f"[Upload] Processing file {file_index + 1}/{len(files)}: {file.filename}")
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞ (PDF, Excel, Word, TXT)
        if not file.filename.endswith(('.txt', '.docx', '.pdf', '.xlsx', '.xls')):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file.filename}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: PDF, Excel (.xlsx, .xls), Word (.docx), TXT"
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–¥ —á—Ç–µ–Ω–∏–µ–º
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ Content-Length, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        file_size = 0
        if hasattr(file, 'size') and file.size:
            file_size = file.size
        elif hasattr(file, 'headers') and 'content-length' in file.headers:
            try:
                file_size = int(file.headers['content-length'])
            except (ValueError, TypeError):
                pass
        
        file_type = file.filename.split('.')[-1].lower()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        file_size_mb = 0
        if hasattr(file, 'size') and file.size:
            file_size_mb = file.size / 1024 / 1024
        logger.info(f"[Upload] File {file.filename}: type={file_type}, size={file_size_mb:.2f}MB")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ (streaming)
        import tempfile
        import shutil
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –¥–µ—Ä–∂–∞—Ç—å –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏
        temp_file = None
        temp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as temp_file:
                temp_path = temp_file.name
                total_size = 0
                chunk_count = 0
                
                logger.info(f"[Upload] Writing file to temp: {temp_path}")
                
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                while True:
                    chunk = await file.read(8192)  # –ß–∏—Ç–∞–µ–º –ø–æ 8KB
                    if not chunk:
                        break
                    total_size += len(chunk)
                    chunk_count += 1
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤–æ –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è
                    if total_size > MAX_FILE_SIZE:
                        os.unlink(temp_path)
                        logger.error(f"[Upload] File {file.filename} too large: {total_size / 1024 / 1024:.2f}MB")
                        raise HTTPException(
                            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                            detail=f"–§–∞–π–ª {file.filename} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({total_size / 1024 / 1024:.2f}MB). –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {MAX_FILE_SIZE / 1024 / 1024:.2f}MB"
                        )
                    
                    temp_file.write(chunk)
                
                temp_file.flush()
                temp_file.close()
                
                logger.info(f"[Upload] File written to temp: {total_size / 1024 / 1024:.2f}MB, {chunk_count} chunks")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏
                current_memory = process.memory_info().rss / 1024 / 1024
                logger.info(f"[Upload] Memory after writing file: {current_memory:.2f}MB (delta: {current_memory - initial_memory:.2f}MB)")
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ë–î —Å—Ä–∞–∑—É
            from app.models.document import Document
            document = Document(
                project_id=project_id,
                filename=file.filename,
                content="–û–±—Ä–∞–±–æ—Ç–∫–∞...",  # –í—Ä–µ–º–µ–Ω–Ω—ã–π placeholder
                file_type=file_type
            )
            db.add(document)
            await db.commit()
            await db.refresh(document)
            
            documents.append(document)
            
            # –ö–†–ò–¢–ò–ß–ù–û: –î–ª—è ma≈Çych plik√≥w (< 5MB) zapisujemy content synchronicznie PRZED Celery
            # To pozwala RAG u≈ºyƒá dokumentu natychmiast
            SMALL_FILE_THRESHOLD = 5 * 1024 * 1024  # 5MB
            if total_size < SMALL_FILE_THRESHOLD:
                try:
                    logger.info(f"[Upload] Small file detected ({total_size / 1024 / 1024:.2f}MB), parsing synchronously for immediate RAG availability")
                    from app.documents.parser import DocumentParser
                    parser = DocumentParser()
                    
                    # Czytamy plik ponownie dla parsowania
                    with open(temp_path, 'rb') as f:
                        file_content = f.read()
                    
                    # Parsujemy synchronicznie
                    text = await parser.parse(file_content, file_type)
                    
                    # Zapisujemy content natychmiast
                    MAX_CONTENT_SIZE = 2_000_000
                    if len(text) > MAX_CONTENT_SIZE:
                        document.content = text[:MAX_CONTENT_SIZE] + f"\n\n[... –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω, –≤—Å–µ–≥–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ...]"
                    else:
                        document.content = text
                    
                    await db.commit()
                    await db.refresh(document)
                    logger.info(f"[Upload] ‚úÖ‚úÖ‚úÖ –î–û–ö–£–ú–ï–ù–¢ –ì–û–¢–û–í –î–õ–Ø RAG –ó–ê–ü–†–û–°–û–í ‚úÖ‚úÖ‚úÖ")
                    logger.info(f"[Upload] üìÑ Document ID: {document.id}")
                    logger.info(f"[Upload] üìÑ Filename: {filename}")
                    logger.info(f"[Upload] üìÑ Content saved synchronously - {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                    logger.info(f"[Upload] üìÑ Document ready for RAG - –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã!")
                    
                    # Usuwamy file_content z pamiƒôci
                    del file_content
                    del text
                except Exception as sync_error:
                    logger.warning(f"[Upload] Synchronous parsing failed, will use Celery: {sync_error}")
                    # Je≈õli synchroniczne parsowanie siƒô nie powiod≈Ço, u≈ºywamy Celery
                    document.content = "–û–±—Ä–∞–±–æ—Ç–∫–∞..."
                    await db.commit()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ Celery –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –≤–æ—Ä–∫–µ—Ä–µ
            # –î–ª—è ma≈Çych plik√≥w Celery tylko przetworzy chunks, content ju≈º jest zapisany
            # –î–ª—è du≈ºych plik√≥w Celery przetworzy wszystko
            from app.tasks.document_tasks import process_document_task
            logger.info(f"[Upload] Scheduling Celery task for document {document.id}, temp_file: {temp_path}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ Celery –æ—á–µ—Ä–µ–¥—å
            # –ó–∞–¥–∞—á–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –≤–æ—Ä–∫–µ—Ä–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å
            try:
                task_result = process_document_task.delay(
                    str(document.id),
                    str(project_id),
                    temp_path,
                    file.filename,
                    file_type
                )
                logger.info(f"[Upload] ‚úÖ Celery task created successfully:")
                logger.info(f"  - Task ID: {task_result.id}")
                logger.info(f"  - Document ID: {document.id}")
                logger.info(f"  - Filename: {file.filename}")
                logger.info(f"  - File type: {file_type}")
                logger.info(f"  - Temp file: {temp_path}")
                logger.info(f"  - Task state: {task_result.state}")
            except Exception as celery_error:
                logger.error(f"[Upload] ‚ùå Failed to create Celery task for document {document.id}: {celery_error}", exc_info=True)
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—à–∏–±–∫—É
                document.content = f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(celery_error)[:200]}"
                await db.commit()
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {str(celery_error)}"
                )
        except HTTPException:
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–∞, —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –µ—Å–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω
            if temp_file and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except:
                    pass
            raise
        except Exception as e:
            # –ï—Å–ª–∏ –¥—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞, —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if temp_file and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except:
                    pass
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {str(e)}"
            )
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏
    final_memory = process.memory_info().rss / 1024 / 1024
    logger.info(f"[Upload] Upload complete: {len(documents)} documents created, final memory: {final_memory:.2f}MB (delta: {final_memory - initial_memory:.2f}MB)")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å—Ä–∞–∑—É (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –≤ —Ñ–æ–Ω–µ)
    return documents


@router.get("/{document_id}/status")
async def get_document_status(
    document_id: UUID,
    db: AsyncSession = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    from app.models.document import Document
    from sqlalchemy import select
    
    result = await db.execute(select(Document).where(Document.id == document_id))
    document = result.scalar_one_or_none()
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        )
    
    content_length = len(document.content) if document.content else 0
    is_processing = document.content in ["–û–±—Ä–∞–±–æ—Ç–∫–∞...", "–û–±—Ä–∞–±–æ—Ç–∞–Ω", ""] or content_length < 100
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —á–∞–Ω–∫–æ–≤
    from app.models.document import DocumentChunk
    chunks_result = await db.execute(
        select(DocumentChunk).where(DocumentChunk.document_id == document_id)
    )
    chunks_count = len(chunks_result.scalars().all())
    
    return {
        "document_id": str(document.id),
        "filename": document.filename,
        "file_type": document.file_type,
        "is_processing": is_processing,
        "content_length": content_length,
        "chunks_count": chunks_count,
        "status": "processing" if is_processing else "ready",
        "content_preview": document.content[:200] if document.content and len(document.content) > 0 else None
    }


@router.get("/{project_id}", response_model=List[DocumentResponse])
async def get_project_documents(
    project_id: UUID,
    db: AsyncSession = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ - –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ chunks)"""
    from sqlalchemy.orm import noload
    from app.models.document import Document
    from sqlalchemy import select
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ chunks –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    result = await db.execute(
        select(Document)
        .where(Document.project_id == project_id)
        .options(noload(Document.chunks))
        .order_by(Document.created_at.desc())
    )
    documents = list(result.scalars().all())
    
    return documents


@router.delete("/{document_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_document(
    document_id: UUID,
    db: AsyncSession = Depends(get_db),
    current_admin = Depends(get_current_admin)
):
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"""
    service = DocumentService(db)
    success = await service.delete_document(document_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        )


