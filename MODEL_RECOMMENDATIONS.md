# Рекомендации по использованию моделей LLM в RAG системе

## Обзор

Система использует OpenRouter для доступа к различным моделям LLM. Все запросы проходят через OpenRouter, что обеспечивает прозрачную оплату и единый API.

## Рекомендуемые модели для разных задач

### 1. Генерация Summary (Резюме документов)

**Рекомендуемые модели:**
- `x-ai/grok-4.1-fast` - быстрая и точная модель для summary
- `deepseek/deepseek-chat` - отличная поддержка русского языка
- `qwen/qwen-2.5-72b-instruct` - хорошая точность для длинных документов

**Рекомендации:**
- **Объем текста:** До 100,000 символов за один запрос
- **Для длинных документов:** Система автоматически использует стратегию анализа по частям (начало, середина, конец)
- **Temperature:** 0.2 (низкая для максимальной точности)
- **Max tokens:** 500 (достаточно для подробного summary)

**Формат промпта:**
```
Проанализируй весь документ и создай точное краткое содержание (summary) на русском языке.

КРИТИЧЕСКИ ВАЖНЫЕ ТРЕБОВАНИЯ:
1. Язык: ТОЛЬКО русский язык
2. Длина: 300-600 символов
3. Точность: Отрази ВСЕ основные темы и ключевую информацию
4. Минимальные искажения: Сохрани точность фактов, цифр, дат, имен собственных
5. Полнота: Упомяни все важные аспекты документа
```

### 2. Ответы на вопросы (RAG)

**Рекомендуемые модели:**
- `x-ai/grok-4.1-fast` - быстрая и точная (primary)
- `openai/gpt-oss-120b:free` - бесплатная fallback модель
- `deepseek/deepseek-chat` - отличная поддержка русского

**Рекомендации:**
- **Объем контекста:** До 10,000 символов из релевантных чанков
- **Temperature:** 0.7 (баланс между точностью и креативностью)
- **Max tokens:** Зависит от max_response_length проекта (обычно 1000-2000)

**Формат промпта:**
```
Контекст из документов:
{chunks}

Вопрос пользователя: {question}

Правила:
1. Отвечай на основе контекста из документов
2. Если информации нет в контексте, честно об этом скажи
3. Будь кратким и структурированным
4. Максимальная длина ответа: {max_length} символов
```

### 3. Описание содержания документов

**Рекомендуемые модели:**
- `x-ai/grok-4.1-fast` - для быстрых ответов
- `qwen/qwen-2.5-72b-instruct` - для более глубокого анализа

**Рекомендации:**
- **Объем:** Анализирует все документы проекта
- **Temperature:** 0.5-0.7
- **Max tokens:** 1500-2000

### 4. Генерация предложенных вопросов

**Рекомендуемые модели:**
- `x-ai/grok-4.1-fast` - быстрая генерация
- `deepseek/deepseek-chat` - креативные вопросы

**Рекомендации:**
- **Temperature:** 0.8 (выше для креативности)
- **Max tokens:** 500
- **Контекст:** Первые 2000 символов из документов

## Ограничения по объему текста

### Для Summary:
- **Короткие документы (< 100k символов):** Анализируются целиком
- **Длинные документы (> 100k символов):** Используется стратегия анализа по частям:
  - Начало документа (первые 33k символов)
  - Середина документа (33k символов из центра)
  - Конец документа (последние 33k символов)

### Для RAG ответов:
- **Максимум чанков:** 10 релевантных чанков
- **Размер чанка:** До 500 символов
- **Общий контекст:** До 10,000 символов

### Для описания содержания:
- **Количество документов:** До 10 последних документов
- **Общий объем:** Зависит от размера документов

## Рекомендации по формату промптов

### 1. Всегда указывайте язык:
```
Отвечай на русском языке
```

### 2. Указывайте требования к формату:
```
- Без маркеров или нумерации
- Сплошной текст
- Структурированный ответ
```

### 3. Указывайте ограничения:
```
- Максимальная длина: {max_length} символов
- Будь кратким и информативным
```

### 4. Указывайте приоритеты:
```
1. Используй информацию из документов
2. Если информации нет, честно об этом скажи
3. Не добавляй информацию, которой нет в документах
```

## Настройка моделей в системе

### Через панель управления:
1. Перейдите в раздел "Управление моделями"
2. Выберите Primary модель (основная)
3. Выберите Fallback модель (резервная)
4. Сохраните настройки

### Через API:
```bash
PATCH /api/models/global-settings
{
  "primary_model_id": "x-ai/grok-4.1-fast",
  "fallback_model_id": "openai/gpt-oss-120b:free"
}
```

### Для конкретного проекта:
```bash
PATCH /api/models/project/{project_id}?model_id=x-ai/grok-4.1-fast
```

## Мониторинг использования через OpenRouter

Все запросы проходят через OpenRouter, что обеспечивает:
- **Прозрачную оплату:** Все запросы видны в OpenRouter Dashboard
- **Единый API:** Один ключ для всех моделей
- **Автоматический fallback:** Система автоматически переключается на резервные модели

### Просмотр статистики:
1. Откройте https://openrouter.ai/
2. Перейдите в Dashboard
3. Просмотрите статистику использования по моделям

## Рекомендации по оптимизации затрат

1. **Используйте быстрые модели для простых задач:**
   - `x-ai/grok-4.1-fast` для summary и простых ответов
   - `openai/gpt-oss-120b:free` для fallback (бесплатно)

2. **Используйте кэширование:**
   - Summary документов кэшируется в БД
   - RAG ответы кэшируются на 5 минут

3. **Ограничивайте max_tokens:**
   - Summary: 500 tokens
   - Ответы: 1000-2000 tokens (зависит от проекта)

4. **Используйте правильную temperature:**
   - Summary: 0.2 (точность)
   - Ответы: 0.7 (баланс)
   - Вопросы: 0.8 (креативность)

## Типовые запросы в боте

### 1. Ответ на вопрос
Просто задайте вопрос в свободной форме:
```
Какие основные темы в документах?
```

### 2. Резюме блока/документа
Используйте команду:
```
/summary
или
/резюме
```

### 3. Описание содержания документа
Используйте команду:
```
/describe
или
/описание
```

## Логирование

Все запросы к моделям логируются:
- Время выполнения
- Использованная модель
- Количество токенов
- Ошибки (если есть)

Логи доступны в:
- Backend logs (Railway/сервер)
- OpenRouter Dashboard (статистика использования)

